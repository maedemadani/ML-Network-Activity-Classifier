import pandas as pd
import numpy as np
import json
from pathlib import Path
from typing import Dict, List, Any, Tuple
from scipy import stats
from sklearn.utils import resample


class StatisticalAnalyzer:
    """ØªØ­Ù„ÛŒÙ„ Ø¢Ù…Ø§Ø±ÛŒ Ù†ØªØ§ÛŒØ¬ Ù…Ø¯Ù„â€ŒÙ‡Ø§"""

    def __init__(self, config):
        self.config = config
        self.statistical_results = {}

    def _convert_to_serializable(self, obj: Any) -> Any:
        """ØªØ¨Ø¯ÛŒÙ„ Ø§Ù†ÙˆØ§Ø¹ ØºÛŒØ±Ù‚Ø§Ø¨Ù„ Ø³Ø±ÛŒØ§Ù„Ø§ÛŒØ² Ø¨Ù‡ Ø§Ù†ÙˆØ§Ø¹ Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯ Ù¾Ø§ÛŒØªÙˆÙ†"""
        if isinstance(obj, (np.integer, np.int32, np.int64)):
            return int(obj)
        elif isinstance(obj, (np.floating, np.float32, np.float64)):
            return float(obj)
        elif isinstance(obj, (np.bool_)):
            return bool(obj)
        elif isinstance(obj, (np.ndarray)):
            return obj.tolist()
        elif isinstance(obj, pd.Timestamp):
            return obj.isoformat()
        elif isinstance(obj, dict):
            return {key: self._convert_to_serializable(value) for key, value in obj.items()}
        elif isinstance(obj, list):
            return [self._convert_to_serializable(item) for item in obj]
        elif isinstance(obj, tuple):
            return tuple(self._convert_to_serializable(item) for item in obj)
        elif pd.isna(obj):
            return None
        elif isinstance(obj, bool):
            return bool(obj)  # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ bool Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯
        else:
            return obj

    def perform_bootstrap_analysis(self, model_summary: pd.DataFrame) -> Dict[str, Any]:
        """Ø§Ù†Ø¬Ø§Ù… ØªØ­Ù„ÛŒÙ„ Ø¨ÙˆØªâ€ŒØ§Ø³ØªØ±Ù¾ Ø¨Ø±Ø§ÛŒ ÙØ§ØµÙ„Ù‡ Ø§Ø·Ù…ÛŒÙ†Ø§Ù†"""
        bootstrap_results = {}

        for metric in ['f1_minority_mean', 'security_score', 'mean_security_recall',
                       'security_f1', 'accuracy']:
            values = model_summary[metric].values
            bootstrap_ci = self._bootstrap_confidence_interval(values)
            bootstrap_results[metric] = bootstrap_ci

        self.statistical_results['bootstrap'] = bootstrap_results
        return bootstrap_results

    def _bootstrap_confidence_interval(self, values: np.ndarray, confidence: float = 0.95) -> Dict[str, float]:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ ÙØ§ØµÙ„Ù‡ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø¨ÙˆØªâ€ŒØ§Ø³ØªØ±Ù¾"""
        # Ø§Ú¯Ø± ÙÙ‚Ø· ÛŒÚ© Ù…Ù‚Ø¯Ø§Ø± Ø¯Ø§Ø±ÛŒÙ…ØŒ ÙØ§ØµÙ„Ù‡ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ù…Ø¹Ù†ÛŒ Ù†Ø¯Ø§Ø±Ø¯
        if len(values) <= 1:
            return {
                'mean': float(values[0]) if len(values) == 1 else 0.0,
                'std': 0.0,
                'ci_lower': float(values[0]) if len(values) == 1 else 0.0,
                'ci_upper': float(values[0]) if len(values) == 1 else 0.0,
                'confidence_level': confidence,
                'warning': 'Insufficient data for confidence interval'
            }

        bootstrap_stats = []

        for _ in range(self.config.N_BOOTSTRAP_SAMPLES):
            sample = resample(values)
            bootstrap_stats.append(np.mean(sample))

        alpha = (1 - confidence) / 2
        lower = np.percentile(bootstrap_stats, alpha * 100)
        upper = np.percentile(bootstrap_stats, (1 - alpha) * 100)

        return {
            'mean': float(np.mean(values)),
            'std': float(np.std(values)),
            'ci_lower': float(lower),
            'ci_upper': float(upper),
            'confidence_level': confidence
        }

    def perform_pairwise_tests(self, model_summary: pd.DataFrame) -> Dict[str, Any]:
        """Ø§Ù†Ø¬Ø§Ù… Ø¢Ø²Ù…ÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø²ÙˆØ¬ÛŒ Ø¨ÛŒÙ† Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒâ€ŒÙ‡Ø§"""
        pairwise_results = {}

        models = model_summary['model'].unique()
        datasets = model_summary['dataset'].unique()

        for model in models:
            model_data = model_summary[model_summary['model'] == model]
            if len(model_data) < 2:
                continue

            pairwise_results[model] = {}

            # Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù‡Ø± Ø¬ÙØª Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ
            for i, ds1 in enumerate(datasets):
                for j, ds2 in enumerate(datasets):
                    if i >= j:
                        continue

                    ds1_data = model_data[model_data['dataset'] == ds1]
                    ds2_data = model_data[model_data['dataset'] == ds2]

                    if len(ds1_data) > 0 and len(ds2_data) > 0:
                        comparison = self._compare_strategies(
                            ds1_data.iloc[0], ds2_data.iloc[0], ds1, ds2
                        )
                        pairwise_results[model][f'{ds1}_vs_{ds2}'] = comparison

        self.statistical_results['pairwise'] = pairwise_results
        return pairwise_results

    def _compare_strategies(self, strategy1: pd.Series, strategy2: pd.Series,
                            name1: str, name2: str) -> Dict[str, Any]:
        """Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¯Ùˆ Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ"""
        comparison = {
            'strategy1': name1,
            'strategy2': name2,
            'compared_metrics': []
        }

        metrics_to_compare = [
            'f1_minority_mean',
            'security_score',
            'mean_security_recall',
            'security_f1',
            'accuracy'
        ]

        for metric in metrics_to_compare:
            val1 = strategy1[metric]
            val2 = strategy2[metric]

            # Ù…Ø¯ÛŒØ±ÛŒØª Ù…Ù‚Ø§Ø¯ÛŒØ± ÛŒÚ©Ø³Ø§Ù† (Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² ØªÙ‚Ø³ÛŒÙ… Ø¨Ø± ØµÙØ±)
            if val1 == val2:
                comparison['compared_metrics'].append({
                    'metric': metric,
                    f'{name1}_value': float(val1),
                    f'{name2}_value': float(val2),
                    'difference': 0.0,
                    'p_value': 1.0,
                    'significant': False,
                    'effect_size': 0.0,
                    'note': 'identical_values'
                })
                continue

            # Ø¢Ø²Ù…ÙˆÙ† t Ø²ÙˆØ¬ÛŒ (Ø³Ø§Ø¯Ù‡â€ŒØ´Ø¯Ù‡)
            try:
                t_stat, p_value = stats.ttest_rel([val1], [val2])
                effect_size = abs(val1 - val2) / max(np.std([val1, val2]), 0.001)  # Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² ØªÙ‚Ø³ÛŒÙ… Ø¨Ø± ØµÙØ±

                comparison['compared_metrics'].append({
                    'metric': metric,
                    f'{name1}_value': float(val1),
                    f'{name2}_value': float(val2),
                    'difference': float(val1 - val2),
                    'p_value': float(p_value),
                    'significant': bool(p_value < self.config.STATISTICAL_ALPHA),
                    'effect_size': float(effect_size)
                })
            except Exception as e:
                comparison['compared_metrics'].append({
                    'metric': metric,
                    f'{name1}_value': float(val1),
                    f'{name2}_value': float(val2),
                    'difference': float(val1 - val2),
                    'p_value': None,
                    'significant': False,
                    'effect_size': float(abs(val1 - val2)),
                    'error': str(e)
                })

        return comparison

    def save_statistical_results(self, output_dir: Path):
        """Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬ Ø¢Ù…Ø§Ø±ÛŒ"""
        # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Ø§Ù†ÙˆØ§Ø¹ Ù‚Ø§Ø¨Ù„ Ø³Ø±ÛŒØ§Ù„Ø§ÛŒØ²
        serializable_results = self._convert_to_serializable(self.statistical_results)

        # Ø°Ø®ÛŒØ±Ù‡ JSON
        json_path = output_dir / "statistical_analysis.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(serializable_results, f, indent=2, ensure_ascii=False)

        # Ø§ÛŒØ¬Ø§Ø¯ Ú¯Ø²Ø§Ø±Ø´ Ù…ØªÙ†ÛŒ
        self._create_statistical_report(output_dir)

        print(f"ğŸ“ˆ Ù†ØªØ§ÛŒØ¬ Ø¢Ù…Ø§Ø±ÛŒ Ø¯Ø± {output_dir} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù†Ø¯")

    def _create_statistical_report(self, output_dir: Path):
        """Ø§ÛŒØ¬Ø§Ø¯ Ú¯Ø²Ø§Ø±Ø´ Ù…ØªÙ†ÛŒ ØªØ­Ù„ÛŒÙ„ Ø¢Ù…Ø§Ø±ÛŒ"""
        report_path = output_dir / "statistical_report.txt"

        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("=" * 60 + "\n")
            f.write("Ú¯Ø²Ø§Ø±Ø´ ØªØ­Ù„ÛŒÙ„ Ø¢Ù…Ø§Ø±ÛŒ Ù†ØªØ§ÛŒØ¬ Ù…Ø¯Ù„â€ŒÙ‡Ø§\n")
            f.write("=" * 60 + "\n\n")

            # Ù†ØªØ§ÛŒØ¬ Ø¨ÙˆØªâ€ŒØ§Ø³ØªØ±Ù¾
            f.write("ÙØ§ØµÙ„Ù‡ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø¨ÙˆØªâ€ŒØ§Ø³ØªØ±Ù¾ (Û¹ÛµÙª):\n")
            if 'bootstrap' in self.statistical_results:
                for metric, ci in self.statistical_results['bootstrap'].items():
                    f.write(f"  {metric}:\n")
                    f.write(f"    Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†: {ci['mean']:.3f}\n")
                    f.write(f"    Ø§Ù†Ø­Ø±Ø§Ù Ù…Ø¹ÛŒØ§Ø±: {ci['std']:.3f}\n")
                    if 'warning' in ci:
                        f.write(f"    âš ï¸  {ci['warning']}\n")
                    else:
                        f.write(f"    ÙØ§ØµÙ„Ù‡ Ø§Ø·Ù…ÛŒÙ†Ø§Ù†: [{ci['ci_lower']:.3f}, {ci['ci_upper']:.3f}]\n\n")

            # Ù†ØªØ§ÛŒØ¬ Ø¢Ø²Ù…ÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø²ÙˆØ¬ÛŒ
            f.write("Ø¢Ø²Ù…ÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø²ÙˆØ¬ÛŒ Ø¨ÛŒÙ† Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒâ€ŒÙ‡Ø§:\n")
            if 'pairwise' in self.statistical_results:
                for model, comparisons in self.statistical_results['pairwise'].items():
                    f.write(f"\n  Ù…Ø¯Ù„ {model}:\n")
                    for comp_name, comp_data in comparisons.items():
                        f.write(f"    Ù…Ù‚Ø§ÛŒØ³Ù‡ {comp_name}:\n")
                        for metric_comp in comp_data['compared_metrics']:
                            if 'error' in metric_comp:
                                f.write(f"      {metric_comp['metric']}: Ø®Ø·Ø§ - {metric_comp['error']}\n")
                            elif 'note' in metric_comp and metric_comp['note'] == 'identical_values':
                                f.write(f"      {metric_comp['metric']}: Ù…Ù‚Ø§Ø¯ÛŒØ± ÛŒÚ©Ø³Ø§Ù†\n")
                            else:
                                significance = "âœ… Ù…Ø¹Ù†ÛŒâ€ŒØ¯Ø§Ø±" if metric_comp['significant'] else "âŒ ØºÛŒØ± Ù…Ø¹Ù†ÛŒâ€ŒØ¯Ø§Ø±"
                                f.write(
                                    f"      {metric_comp['metric']}: p-value={metric_comp['p_value']:.4f} ({significance})\n")