import pandas as pd
import json
from pathlib import Path
from typing import Dict, Any


class ModelSelector:
    """Ø§Ù†ØªØ®Ø§Ø¨â€ŒÚ©Ù†Ù†Ø¯Ù‡ Ù…Ø¯Ù„ Ù†Ù‡Ø§ÛŒÛŒ"""

    def __init__(self, config):
        self.config = config
        self.selection_results = {}

    def select_best_model(self, model_summary: pd.DataFrame) -> Dict[str, Any]:
        """Ø§Ù†ØªØ®Ø§Ø¨ Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ ØªØ¹Ø±ÛŒÙ Ø´Ø¯Ù‡"""
        print("ğŸ† Ø¯Ø± Ø­Ø§Ù„ Ø§Ù†ØªØ®Ø§Ø¨ Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„...")

        # ÙÛŒÙ„ØªØ± Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ minimum
        filtered_models = self._filter_models_by_criteria(model_summary)

        if filtered_models.empty:
            print("âš ï¸  Ù‡ÛŒÚ† Ù…Ø¯Ù„ÛŒ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ minimum Ø±Ø§ Ù†Ø¯Ø§Ø´Øª. Ø¯Ø± Ø­Ø§Ù„ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù‡Ù…Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§...")
            filtered_models = model_summary

        # Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§
        ranked_models = self._rank_models(filtered_models)

        # Ø§Ù†ØªØ®Ø§Ø¨ Ù…Ø¯Ù„ Ø¨Ø±ØªØ± Ùˆ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù¾Ø´ØªÛŒØ¨Ø§Ù†
        best_model = ranked_models.iloc[0]
        runner_ups = ranked_models.iloc[1:3] if len(ranked_models) > 1 else pd.DataFrame()

        selection_info = {
            'selected_model': self._format_model_info(best_model),
            'runner_ups': [self._format_model_info(model) for _, model in runner_ups.iterrows()],
            'selection_criteria_used': self.config.SELECTION_CRITERIA,
            'ranking_metrics': ['security_score', 'f1_minority_mean', 'recall_minority_mean']
        }

        # ØªØ­Ù„ÛŒÙ„ trade-off
        selection_info['trade_off_analysis'] = self._analyze_trade_offs(best_model, model_summary)

        self.selection_results = selection_info
        return selection_info

    def _filter_models_by_criteria(self, df: pd.DataFrame) -> pd.DataFrame:
        """ÙÛŒÙ„ØªØ± Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ minimum"""
        criteria = self.config.SELECTION_CRITERIA

        filtered = df.copy()

        if 'f1_minority_mean' in criteria:
            filtered = filtered[filtered['f1_minority_mean'] >= criteria['f1_minority_mean']]

        if 'min_recall_minority' in criteria:
            filtered = filtered[filtered['recall_minority_mean'] >= criteria['min_recall_minority']]

        if 'max_inference_time_ms' in criteria:
            filtered = filtered[filtered['inference_time_ms'] <= criteria['max_inference_time_ms']]

        if 'max_model_size_mb' in criteria:
            filtered = filtered[filtered['model_size_mb'] <= criteria['max_model_size_mb']]

        return filtered

    def _rank_models(self, df: pd.DataFrame) -> pd.DataFrame:
        """Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§"""
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² Ù†Ù‡Ø§ÛŒÛŒ
        df['final_score'] = (
                df['security_score'] * 0.4 +
                df['f1_minority_mean'] * 0.3 +
                df['recall_minority_mean'] * 0.2 +
                df['threat_detection_rate'] * 0.1
        )

        # Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ù†Ø²ÙˆÙ„ÛŒ
        ranked = df.sort_values('final_score', ascending=False)
        return ranked

    def _format_model_info(self, model_row: pd.Series) -> Dict[str, Any]:
        """ÙØ±Ù…Øªâ€ŒØ¯Ù‡ÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ø¯Ù„"""
        return {
            'model': model_row['model'],
            'dataset': model_row['dataset'],
            'metrics': {
                'accuracy': float(model_row['accuracy']),
                'f1_macro': float(model_row['f1_macro']),
                'f1_minority_mean': float(model_row['f1_minority_mean']),
                'recall_minority_mean': float(model_row['recall_minority_mean']),
                'security_score': float(model_row['security_score']),
                'threat_detection_rate': float(model_row['threat_detection_rate'])
            },
            'minority_class_performance': {
                f'class_{cls}': {
                    'f1': float(model_row[f'f1_class_{cls}']),
                    'recall': float(model_row[f'recall_class_{cls}']),
                    'precision': float(model_row[f'precision_class_{cls}'])
                } for cls in self.config.MINORITY_CLASSES
            }
        }

    def _analyze_trade_offs(self, selected_model: pd.Series, all_models: pd.DataFrame) -> Dict[str, Any]:
        """ØªØ­Ù„ÛŒÙ„ trade-offâ€ŒÙ‡Ø§ÛŒ Ù…Ø¯Ù„ Ø§Ù†ØªØ®Ø§Ø¨â€ŒØ´Ø¯Ù‡"""
        best_accuracy = all_models['accuracy'].max()
        best_f1_macro = all_models['f1_macro'].max()

        trade_offs = {
            'accuracy_tradeoff': float(best_accuracy - selected_model['accuracy']),
            'f1_macro_tradeoff': float(best_f1_macro - selected_model['f1_macro']),
            'security_improvement': float(selected_model['security_score'] - all_models['security_score'].mean()),
            'explanation': self._generate_tradeoff_explanation(selected_model, best_accuracy, best_f1_macro)
        }

        return trade_offs

    def _generate_tradeoff_explanation(self, selected_model: pd.Series,
                                       best_accuracy: float, best_f1_macro: float) -> str:
        """ØªÙˆÙ„ÛŒØ¯ ØªÙˆØ¶ÛŒØ­ trade-off"""
        accuracy_diff = best_accuracy - selected_model['accuracy']
        f1_diff = best_f1_macro - selected_model['f1_macro']

        explanations = []

        if accuracy_diff > 0.05:
            explanations.append(f"Ø¯Ù‚Øª Ú©Ù„ÛŒ {accuracy_diff:.3f} Ú©Ù…ØªØ± Ø§Ø² Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„ Ø§Ø³Øª")
        elif accuracy_diff > 0.02:
            explanations.append(f"Ø¯Ù‚Øª Ú©Ù„ÛŒ Ú©Ù…ÛŒ ({accuracy_diff:.3f}) Ú©Ù…ØªØ± Ø§Ø³Øª")
        else:
            explanations.append("Ø¯Ù‚Øª Ú©Ù„ÛŒ ØªÙ‚Ø±ÛŒØ¨Ø§Ù‹ Ø¨Ù‡ÛŒÙ†Ù‡ Ø§Ø³Øª")

        if f1_diff > 0.05:
            explanations.append(f"F1 Ú©Ù„ÛŒ {f1_diff:.3f} Ú©Ù…ØªØ± Ø§Ø² Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„ Ø§Ø³Øª")
        elif f1_diff > 0.02:
            explanations.append(f"F1 Ú©Ù„ÛŒ Ú©Ù…ÛŒ ({f1_diff:.3f}) Ú©Ù…ØªØ± Ø§Ø³Øª")
        else:
            explanations.append("F1 Ú©Ù„ÛŒ ØªÙ‚Ø±ÛŒØ¨Ø§Ù‹ Ø¨Ù‡ÛŒÙ†Ù‡ Ø§Ø³Øª")

        security_improvement = selected_model['security_score'] - selected_model['f1_macro']
        if security_improvement > 0.1:
            explanations.append("Ø§Ù…ØªÛŒØ§Ø² Ø§Ù…Ù†ÛŒØªÛŒ Ø¨Ù‡ Ø·ÙˆØ± Ù‚Ø§Ø¨Ù„ ØªÙˆØ¬Ù‡ÛŒ Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡ Ø§Ø³Øª")
        elif security_improvement > 0.05:
            explanations.append("Ø§Ù…ØªÛŒØ§Ø² Ø§Ù…Ù†ÛŒØªÛŒ moderately Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡ Ø§Ø³Øª")
        else:
            explanations.append("ØªØ¹Ø§Ø¯Ù„ Ø®ÙˆØ¨ÛŒ Ø¨ÛŒÙ† Ø§Ù…Ù†ÛŒØª Ùˆ Ø¯Ù‚Øª Ú©Ù„ÛŒ Ø¨Ø±Ù‚Ø±Ø§Ø± Ø§Ø³Øª")

        return ". ".join(explanations)

    def save_selection_results(self, output_dir: Path):
        """Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬ Ø§Ù†ØªØ®Ø§Ø¨"""
        json_path = output_dir / "selected_model.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(self.selection_results, f, indent=2, ensure_ascii=False)

        # Ø§ÛŒØ¬Ø§Ø¯ Ú¯Ø²Ø§Ø±Ø´ Ù…ØªÙ†ÛŒ
        self._create_selection_report(output_dir)

        print(f"ğŸ† Ù†ØªØ§ÛŒØ¬ Ø§Ù†ØªØ®Ø§Ø¨ Ù…Ø¯Ù„ Ø¯Ø± {output_dir} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù†Ø¯")

    def _create_selection_report(self, output_dir: Path):
        """Ø§ÛŒØ¬Ø§Ø¯ Ú¯Ø²Ø§Ø±Ø´ Ù…ØªÙ†ÛŒ Ø§Ù†ØªØ®Ø§Ø¨ Ù…Ø¯Ù„"""
        report_path = output_dir / "model_selection_report.txt"

        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("=" * 60 + "\n")
            f.write("Ú¯Ø²Ø§Ø±Ø´ Ø§Ù†ØªØ®Ø§Ø¨ Ù…Ø¯Ù„ Ù†Ù‡Ø§ÛŒÛŒ\n")
            f.write("=" * 60 + "\n\n")

            selected = self.selection_results['selected_model']
            f.write(f"Ù…Ø¯Ù„ Ø§Ù†ØªØ®Ø§Ø¨â€ŒØ´Ø¯Ù‡: {selected['model']} (Ø¯Ø§Ø¯Ù‡: {selected['dataset']})\n\n")

            f.write("Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯:\n")
            for metric, value in selected['metrics'].items():
                f.write(f"  {metric}: {value:.3f}\n")

            f.write("\nØ¹Ù…Ù„Ú©Ø±Ø¯ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ÛŒ Ø§Ù…Ù†ÛŒØªÛŒ:\n")
            for cls, perf in selected['minority_class_performance'].items():
                f.write(f"  {cls}:\n")
                f.write(f"    F1: {perf['f1']:.3f}\n")
                f.write(f"    Recall: {perf['recall']:.3f}\n")
                f.write(f"    Precision: {perf['precision']:.3f}\n")

            f.write("\nØªØ­Ù„ÛŒÙ„ Trade-off:\n")
            trade_offs = self.selection_results['trade_off_analysis']
            f.write(f"  Ú©Ø§Ù‡Ø´ Ø¯Ù‚Øª: {trade_offs['accuracy_tradeoff']:.3f}\n")
            f.write(f"  Ú©Ø§Ù‡Ø´ F1 Ú©Ù„ÛŒ: {trade_offs['f1_macro_tradeoff']:.3f}\n")
            f.write(f"  Ø¨Ù‡Ø¨ÙˆØ¯ Ø§Ù…Ù†ÛŒØª: {trade_offs['security_improvement']:.3f}\n")
            f.write(f"  ØªÙˆØ¶ÛŒØ­: {trade_offs['explanation']}\n")

            if self.selection_results['runner_ups']:
                f.write("\nÙ…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù¾Ø´ØªÛŒØ¨Ø§Ù†:\n")
                for i, runner_up in enumerate(self.selection_results['runner_ups'], 1):
                    f.write(
                        f"  {i}. {runner_up['model']} ({runner_up['dataset']}) - Ø§Ù…ØªÛŒØ§Ø² Ø§Ù…Ù†ÛŒØªÛŒ: {runner_up['metrics']['security_score']:.3f}\n")