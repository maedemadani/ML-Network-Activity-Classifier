import pandas as pd
import numpy as np
import json
import joblib
from pathlib import Path
from typing import Dict, List, Any, Tuple
from datetime import datetime


class ResultsAggregator:
    """Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ùˆ Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ Ù†ØªØ§ÛŒØ¬ Ù…Ø¯Ù„â€ŒÙ‡Ø§"""

    def __init__(self, config):
        self.config = config
        self.model_summary = pd.DataFrame()
        self.detailed_results = {}

    def load_phase4_results(self, evaluation_path: Path) -> bool:
        """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù†ØªØ§ÛŒØ¬ ÙØ§Ø² Û´ Ùˆ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù†Ø§Ù… Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒâ€ŒÙ‡Ø§"""
        try:
            # Û±. Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ú¯Ø²Ø§Ø±Ø´ Ù…Ù‚Ø§ÛŒØ³Ù‡â€ŒØ§ÛŒ Ú©Ù„ÛŒ
            comp_path = evaluation_path / "comparative_analysis.json"
            with open(comp_path, 'r', encoding='utf-8') as f:
                comparative_data = json.load(f)

            # Û². Ù†Ú¯Ø§Ø´Øª Ù†Ø§Ù…â€ŒÙ‡Ø§ÛŒ Ù‚Ø¯ÛŒÙ…ÛŒ Ø¨Ù‡ Ø¬Ø¯ÛŒØ¯
            name_map = {
                'original': 'baseline',
                'oversampled': 'oversampling',
                'undersampled': 'undersampling',
                'oversampling': 'oversampling',
                'undersampling': 'undersampling',
                'baseline': 'baseline'
            }

            # Û³. Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ú¯Ø²Ø§Ø±Ø´â€ŒÙ‡Ø§ÛŒ ØªÙØµÛŒÙ„ÛŒ
            detailed_data = {}
            for raw_name in ['original', 'undersampled', 'oversampled',
                             'baseline', 'undersampling', 'oversampling']:
                strategy = name_map.get(raw_name, raw_name)
                detail_path = evaluation_path / f"detailed_report_{raw_name}.json"
                if not detail_path.exists():
                    # Ø¨Ø±Ø±Ø³ÛŒ Ø§Ú¯Ø± ÙØ§ÛŒÙ„ Ø¨Ø§ Ù†Ø§Ù… Ù†Ø±Ù…Ø§Ù„ Ø´Ø¯Ù‡ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯
                    alt_path = evaluation_path / f"detailed_report_{strategy}.json"
                    if alt_path.exists():
                        detail_path = alt_path
                    else:
                        continue
                with open(detail_path, 'r', encoding='utf-8') as f:
                    detailed_data[strategy] = json.load(f)

            # Û´. Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¯Ø± Ø­Ø§ÙØ¸Ù‡ Ø¯Ø§Ø®Ù„ÛŒ
            self.detailed_results = {
                'comparative': comparative_data,
                'detailed': detailed_data
            }

            print(f"âœ… Ù†ØªØ§ÛŒØ¬ ÙØ§Ø² Û´ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯Ù†Ø¯ ({len(detailed_data)} Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ ÛŒØ§ÙØª Ø´Ø¯)")
            return True

        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù†ØªØ§ÛŒØ¬ ÙØ§Ø² Û´: {e}")
            return False

    def create_model_summary_table(self) -> pd.DataFrame:
        """Ø§ÛŒØ¬Ø§Ø¯ Ø¬Ø¯ÙˆÙ„ Ø®Ù„Ø§ØµÙ‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§"""
        summary_data = []

        for strategy, models_data in self.detailed_results['detailed'].items():
            for model_name, model_results in models_data.items():
                if 'general_metrics' in model_results:
                    row = self._extract_model_metrics(strategy, model_name, model_results)
                    summary_data.append(row)

        self.model_summary = pd.DataFrame(summary_data)

        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ ØªØ±Ú©ÛŒØ¨ÛŒ
        self._calculate_composite_metrics()

        return self.model_summary

    def _extract_model_metrics(self, strategy: str, model_name: str,
                               model_results: Dict[str, Any]) -> Dict[str, Any]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ù‡Ø± Ù…Ø¯Ù„"""
        general_metrics = model_results['general_metrics']
        security_metrics = model_results['security_metrics']

        row = {
            'model': model_name,
            'dataset': strategy,
            'accuracy': general_metrics['accuracy'],
            'f1_macro': general_metrics['f1_macro'],
            'f1_weighted': general_metrics['f1_weighted'],
            'precision_macro': general_metrics['precision_macro'],
            'recall_macro': general_metrics['recall_macro'],
            'inference_time_ms': 0,  # Placeholder - needs actual measurement
            'model_size_mb': 0,  # Placeholder - needs actual measurement
        }

        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ÛŒ Ø§Ù…Ù†ÛŒØªÛŒ
        for cls in self.config.MINORITY_CLASSES:
            row[f'f1_class_{cls}'] = general_metrics.get(f'f1_class_{cls}', 0)
            row[f'recall_class_{cls}'] = general_metrics.get(f'recall_class_{cls}', 0)
            row[f'precision_class_{cls}'] = general_metrics.get(f'precision_class_{cls}', 0)

        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ø§Ù…Ù†ÛŒØªÛŒ ØªØ±Ú©ÛŒØ¨ÛŒ
        row['mean_security_recall'] = security_metrics['mean_security_recall']
        row['security_f1'] = security_metrics['security_f1']
        row['threat_detection_rate'] = security_metrics['threat_detection_rate']

        return row

    def _calculate_composite_metrics(self):
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ ØªØ±Ú©ÛŒØ¨ÛŒ"""
        # Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† F1 Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ÛŒ Ø§Ù‚Ù„ÛŒØª
        f1_minority_cols = [f'f1_class_{cls}' for cls in self.config.MINORITY_CLASSES]
        self.model_summary['f1_minority_mean'] = self.model_summary[f1_minority_cols].mean(axis=1)

        # Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Recall Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ÛŒ Ø§Ù‚Ù„ÛŒØª
        recall_minority_cols = [f'recall_class_{cls}' for cls in self.config.MINORITY_CLASSES]
        self.model_summary['recall_minority_mean'] = self.model_summary[recall_minority_cols].mean(axis=1)

        # Ø§Ù…ØªÛŒØ§Ø² Ø§Ù…Ù†ÛŒØªÛŒ ØªØ±Ú©ÛŒØ¨ÛŒ
        self.model_summary['security_score'] = (
                self.model_summary['f1_minority_mean'] * 0.4 +
                self.model_summary['recall_minority_mean'] * 0.4 +
                self.model_summary['threat_detection_rate'] * 0.2
        )

    def save_summary_tables(self, output_dir: Path):
        """Ø°Ø®ÛŒØ±Ù‡ Ø¬Ø¯Ø§ÙˆÙ„ Ø®Ù„Ø§ØµÙ‡"""
        # Ø°Ø®ÛŒØ±Ù‡ CSV
        csv_path = output_dir / "model_summary.csv"
        self.model_summary.to_csv(csv_path, index=False, encoding='utf-8')

        # Ø°Ø®ÛŒØ±Ù‡ JSON
        json_path = output_dir / "model_summary.json"
        summary_dict = self.model_summary.to_dict('records')
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(summary_dict, f, indent=2, ensure_ascii=False)

        print(f"ğŸ“Š Ø¬Ø¯Ø§ÙˆÙ„ Ø®Ù„Ø§ØµÙ‡ Ø¯Ø± {output_dir} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù†Ø¯")

        unified_csv = output_dir / "final_summary.csv"
        self.model_summary.to_csv(unified_csv, index=False, encoding='utf-8')
        print(f"ğŸ“„ Ø¬Ø¯ÙˆÙ„ Ù†Ù‡Ø§ÛŒÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {unified_csv.name}")

    def get_top_models(self, n: int = 5) -> pd.DataFrame:
        """Ø¯Ø±ÛŒØ§ÙØª Ø¨Ø±ØªØ±ÛŒÙ† Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ù…ØªÛŒØ§Ø² Ø§Ù…Ù†ÛŒØªÛŒ"""
        return self.model_summary.nlargest(n, 'security_score')