import pandas as pd
import numpy as np
import json
from pathlib import Path
from sklearn.preprocessing import StandardScaler, RobustScaler, LabelEncoder
from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_classif
from sklearn.decomposition import PCA
import warnings

warnings.filterwarnings('ignore')


class SmartFeatureEngineer:
    """Ú©Ù„Ø§Ø³ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ù…Ù‡Ù†Ø¯Ø³ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒ Ø¨Ø±Ø§ÛŒ Ø¯ÛŒØªØ§Ø³Øªâ€ŒÙ‡Ø§ÛŒ Ø´Ø¨Ú©Ù‡"""

    def __init__(self, metadata_path, target_column='Action', scaling_strategy='standard'):
        self.metadata = self._load_metadata(metadata_path)
        self.target_column = target_column
        self.scaling_strategy = scaling_strategy
        self.detected_features = {}
        self.scalers = {}
        self.encoders = {}
        self.feature_selector = None
        self.fitted = False

    def _load_metadata(self, metadata_path):
        """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…ØªØ§Ø¯ÛŒØªØ§ÛŒ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ Ø§Ø² ÙØ§Ø² Û±"""
        try:
            with open(metadata_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            raise Exception(f"Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…ØªØ§Ø¯ÛŒØªØ§: {e}")

    def _find_columns_by_pattern(self, patterns):
        """Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ù†Ø§Ù… - Ø¹Ù…ÙˆÙ…ÛŒ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø¯ÛŒØªØ§Ø³Øª"""
        found_columns = []
        for col in self.metadata['all_columns']:
            if col == self.target_column:
                continue
            col_lower = col.lower()
            for pattern in patterns:
                if pattern.lower() in col_lower:
                    found_columns.append(col)
                    break
        return list(set(found_columns))  # Ø­Ø°Ù Ù…ÙˆØ§Ø±Ø¯ ØªÚ©Ø±Ø§Ø±ÛŒ

    def detect_available_features(self):
        """ØªØ´Ø®ÛŒØµ Ø®ÙˆØ¯Ú©Ø§Ø± Ø§Ù†ÙˆØ§Ø¹ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± dataset"""
        print("ğŸ” Ø¯Ø± Ø­Ø§Ù„ ØªØ´Ø®ÛŒØµ Ø®ÙˆØ¯Ú©Ø§Ø± ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯...")

        self.detected_features = {
            # Ù¾ÙˆØ±Øªâ€ŒÙ‡Ø§
            'source_ports': self._find_columns_by_pattern(['source port', 'src_port', 'src port']),
            'destination_ports': self._find_columns_by_pattern(['destination port', 'dst_port', 'dst port']),
            'nat_ports': self._find_columns_by_pattern(['nat source', 'nat destination', 'nat_port']),

            # ØªØ±Ø§ÙÛŒÚ©
            'bytes_columns': self._find_columns_by_pattern(['bytes', 'byte']),
            'packets_columns': self._find_columns_by_pattern(['packets', 'packet']),
            'sent_columns': self._find_columns_by_pattern(['sent', 'forward']),
            'received_columns': self._find_columns_by_pattern(['received', 'backward']),

            # Ø²Ù…Ø§Ù†
            'time_columns': self._find_columns_by_pattern(['time', 'duration', 'elapsed', 'interval']),
            'timestamp_columns': self._find_columns_by_pattern(['timestamp', 'date', 'time']),
        }

        # Ú¯Ø²Ø§Ø±Ø´ ØªØ´Ø®ÛŒØµ
        print("âœ… ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù‡:")
        for feature_type, columns in self.detected_features.items():
            if columns:
                print(f"   ğŸ“Š {feature_type}: {columns}")

        return self.detected_features

    def fit_transform(self, df, train_mask=None):
        """Ø¢Ù…ÙˆØ²Ø´ Ùˆ ØªØ¨Ø¯ÛŒÙ„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ - Ø¨Ø±Ø§ÛŒ Train set"""
        print("ğŸš€ Ø´Ø±ÙˆØ¹ ÙØ±Ø¢ÛŒÙ†Ø¯ Ù…Ù‡Ù†Ø¯Ø³ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ ")
        print("=" * 60)

        self.fitted = True
        df_processed = df.copy()

        # ØªØ´Ø®ÛŒØµ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§
        self.detect_available_features()

        # Û±. Ø§ÛŒØ¬Ø§Ø¯ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ NAT
        df_processed = self._create_nat_features(df_processed)

        # Û². Ù…Ù‡Ù†Ø¯Ø³ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù¾ÙˆØ±ØªÛŒ
        df_processed = self._engineer_port_features(df_processed)

        # Û³. Ù…Ù‡Ù†Ø¯Ø³ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ ØªØ±Ø§ÙÛŒÚ©ÛŒ
        df_processed = self._engineer_traffic_features(df_processed)

        # Û´. Ù…Ù‡Ù†Ø¯Ø³ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø²Ù…Ø§Ù†ÛŒ
        df_processed = self._engineer_time_features(df_processed)

        # Ûµ. ØªØ¨Ø¯ÛŒÙ„ Ù„Ú¯Ø§Ø±ÛŒØªÙ…ÛŒ
        df_processed = self._apply_log_transforms(df_processed)

        # Û¶. Ø±Ù…Ø²Ú¯Ø°Ø§Ø±ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ
        df_processed = self._encode_categorical_features(df_processed)

        # Û·. Ù…Ù‚ÛŒØ§Ø³â€ŒØ¨Ù†Ø¯ÛŒ (ÙÙ‚Ø· Ø±ÙˆÛŒ train)
        if train_mask is not None:
            df_train = df_processed[train_mask].copy()
            df_train = self._scale_features(df_train, fit_scalers=True)
            df_processed.loc[train_mask] = df_train
        else:
            df_processed = self._scale_features(df_processed, fit_scalers=True)

        # Û¸. Ø§Ù†ØªØ®Ø§Ø¨ ÙˆÛŒÚ˜Ú¯ÛŒ
        df_processed = self._select_features(df_processed)

        print(f"âœ… Ù…Ù‡Ù†Ø¯Ø³ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ completed! Ø§Ø¨Ø¹Ø§Ø¯ Ù†Ù‡Ø§ÛŒÛŒ: {df_processed.shape}")
        return df_processed

    def transform(self, df):
        """ØªØ¨Ø¯ÛŒÙ„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ - Ø¨Ø±Ø§ÛŒ Test set"""
        if not self.fitted:
            raise Exception("Ø§Ø¨ØªØ¯Ø§ Ø¨Ø§ÛŒØ¯ Ù…ØªØ¯ fit_transform ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ Ø´ÙˆØ¯")

        print("ğŸ”§ ØªØ¨Ø¯ÛŒÙ„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ - Ø­Ø§Ù„Øª Ø¢Ø²Ù…ÙˆÙ†")
        df_processed = df.copy()

        # Û±. Ø§ÛŒØ¬Ø§Ø¯ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ NAT
        df_processed = self._create_nat_features(df_processed)

        # Û². Ù…Ù‡Ù†Ø¯Ø³ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù¾ÙˆØ±ØªÛŒ
        df_processed = self._engineer_port_features(df_processed)

        # Û³. Ù…Ù‡Ù†Ø¯Ø³ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ ØªØ±Ø§ÙÛŒÚ©ÛŒ
        df_processed = self._engineer_traffic_features(df_processed)

        # Û´. Ù…Ù‡Ù†Ø¯Ø³ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø²Ù…Ø§Ù†ÛŒ
        df_processed = self._engineer_time_features(df_processed)

        # Ûµ. ØªØ¨Ø¯ÛŒÙ„ Ù„Ú¯Ø§Ø±ÛŒØªÙ…ÛŒ
        df_processed = self._apply_log_transforms(df_processed)

        # Û¶. Ø±Ù…Ø²Ú¯Ø°Ø§Ø±ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ
        df_processed = self._encode_categorical_features(df_processed, is_train=False)

        # Û·. Ù…Ù‚ÛŒØ§Ø³â€ŒØ¨Ù†Ø¯ÛŒ (Ø¨Ø§ scalerÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ø¯ÛŒØ¯Ù‡)
        df_processed = self._scale_features(df_processed, fit_scalers=False)

        # Û¸. Ø§Ù†ØªØ®Ø§Ø¨ ÙˆÛŒÚ˜Ú¯ÛŒ (Ø¨Ø§ selector Ø¢Ù…ÙˆØ²Ø´ Ø¯ÛŒØ¯Ù‡)
        df_processed = self._select_features(df_processed, is_train=False)

        return df_processed

    def _create_nat_features(self, df):
        """Ø§ÛŒØ¬Ø§Ø¯ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ NAT - Ø¹Ù…ÙˆÙ…ÛŒ"""
        print("ğŸ”§ Ø§ÛŒØ¬Ø§Ø¯ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ NAT...")

        nat_features_added = 0

        # Ù¾Ø±Ú†Ù… NAT Ú©Ù„ÛŒ
        nat_port_cols = self.detected_features['nat_ports']
        if nat_port_cols:
            # Ø¨Ø±Ø±Ø³ÛŒ Ø¢ÛŒØ§ Ù‡Ø± Ú©Ø¯Ø§Ù… Ø§Ø² Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ NAT Ù…Ù‚Ø¯Ø§Ø± ØºÛŒØ±ØµÙØ± Ø¯Ø§Ø±Ù†Ø¯
            nat_flags = df[nat_port_cols].fillna(0) != 0
            df['has_nat'] = nat_flags.any(axis=1).astype(int)
            nat_features_added += 1

            # Ù¾Ø±Ú†Ù…â€ŒÙ‡Ø§ÛŒ ØªØ·Ø§Ø¨Ù‚ Ù¾ÙˆØ±Øªâ€ŒÙ‡Ø§ÛŒ NAT Ø¨Ø§ Ø§ØµÙ„ÛŒ
            src_port_cols = self.detected_features['source_ports']
            dst_port_cols = self.detected_features['destination_ports']

            if src_port_cols and len(nat_port_cols) >= 1:
                src_col = src_port_cols[0]
                nat_src_col = [col for col in nat_port_cols if 'source' in col.lower() or 'src' in col.lower()]
                if nat_src_col:
                    df['src_port_nat_match'] = (df[src_col] == df[nat_src_col[0]]).astype(int)
                    nat_features_added += 1

            if dst_port_cols and len(nat_port_cols) >= 1:
                dst_col = dst_port_cols[0]
                nat_dst_col = [col for col in nat_port_cols if 'destination' in col.lower() or 'dst' in col.lower()]
                if nat_dst_col:
                    df['dst_port_nat_match'] = (df[dst_col] == df[nat_dst_col[0]]).astype(int)
                    nat_features_added += 1

        print(f"   âœ… {nat_features_added} ÙˆÛŒÚ˜Ú¯ÛŒ NAT Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯")
        return df

    def _engineer_port_features(self, df):
        """Ù…Ù‡Ù†Ø¯Ø³ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù¾ÙˆØ±ØªÛŒ - Ø¹Ù…ÙˆÙ…ÛŒ"""
        print("ğŸ”§ Ø§ÛŒØ¬Ø§Ø¯ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù¾ÙˆØ±ØªÛŒ...")

        port_features_added = 0

        # Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ù¾ÙˆØ±Øª Ù…Ø¨Ø¯Ø§
        src_port_cols = self.detected_features['source_ports']
        if src_port_cols:
            for col in src_port_cols:
                df[f'{col}_category'] = df[col].apply(self._categorize_port)
                port_features_added += 1

                # Ù¾Ø±Ú†Ù…â€ŒÙ‡Ø§ÛŒ Ø³Ø±ÙˆÛŒØ³ Ù…Ù‡Ù…
                df[f'{col}_is_well_known'] = (df[col] <= 1023).astype(int)
                df[f'{col}_is_ephemeral'] = (df[col] >= 49152).astype(int)
                port_features_added += 2

        # Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ù¾ÙˆØ±Øª Ù…Ù‚ØµØ¯
        dst_port_cols = self.detected_features['destination_ports']
        if dst_port_cols:
            for col in dst_port_cols:
                df[f'{col}_category'] = df[col].apply(self._categorize_port)
                port_features_added += 1

                # Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø³Ø±ÙˆÛŒØ³â€ŒÙ‡Ø§ÛŒ Ø±Ø§ÛŒØ¬
                df[f'{col}_is_http'] = (df[col].isin([80, 8080, 443])).astype(int)
                df[f'{col}_is_dns'] = (df[col] == 53).astype(int)
                df[f'{col}_is_ssh'] = (df[col] == 22).astype(int)
                port_features_added += 3

        print(f"   âœ… {port_features_added} ÙˆÛŒÚ˜Ú¯ÛŒ Ù¾ÙˆØ±ØªÛŒ Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯")
        return df

    def _categorize_port(self, port):
        """Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ù¾ÙˆØ±Øª - Ø¹Ù…ÙˆÙ…ÛŒ"""
        try:
            port = int(port)
            if port <= 1023:
                return 'well_known'
            elif port <= 49151:
                return 'registered'
            else:
                return 'ephemeral'
        except:
            return 'unknown'

    def _engineer_traffic_features(self, df):
        """Ù…Ù‡Ù†Ø¯Ø³ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ ØªØ±Ø§ÙÛŒÚ©ÛŒ - Ø¹Ù…ÙˆÙ…ÛŒ"""
        print("ğŸ”§ Ø§ÛŒØ¬Ø§Ø¯ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ ØªØ±Ø§ÙÛŒÚ©ÛŒ...")

        traffic_features_added = 0
        epsilon = 1e-8

        # Ù†Ø³Ø¨Øªâ€ŒÙ‡Ø§ÛŒ Ø§Ø±Ø³Ø§Ù„/Ø¯Ø±ÛŒØ§ÙØª
        bytes_cols = self.detected_features['bytes_columns']
        sent_cols = self.detected_features['sent_columns']
        received_cols = self.detected_features['received_columns']
        packets_cols = self.detected_features['packets_columns']

        # Ù†Ø³Ø¨Øª Ø¨Ø§ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ø§Ø±Ø³Ø§Ù„ÛŒ
        if len(bytes_cols) >= 1 and len(sent_cols) >= 1:
            total_bytes_col = bytes_cols[0]
            sent_bytes_col = sent_cols[0]
            df['bytes_sent_ratio'] = df[sent_bytes_col] / (df[total_bytes_col] + epsilon)
            traffic_features_added += 1

        # Ù†Ø³Ø¨Øª Ù¾Ú©Øªâ€ŒÙ‡Ø§ÛŒ Ø§Ø±Ø³Ø§Ù„ÛŒ
        if len(packets_cols) >= 1 and len(sent_cols) >= 1:
            # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ø³ØªÙˆÙ† pkts_sent
            pkts_sent_cols = self._find_columns_by_pattern(['pkts_sent', 'packets_sent'])
            pkts_received_cols = self._find_columns_by_pattern(['pkts_received', 'packets_received'])

            if pkts_sent_cols and pkts_received_cols:
                df['pkts_sent_ratio'] = df[pkts_sent_cols[0]] / (
                            df[pkts_sent_cols[0]] + df[pkts_received_cols[0]] + epsilon)
                traffic_features_added += 1

        # Ú©Ø§Ø±Ø§ÛŒÛŒ ØªØ±Ø§ÙÛŒÚ©
        if len(bytes_cols) >= 1 and len(packets_cols) >= 1:
            df['avg_packet_size'] = df[bytes_cols[0]] / (df[packets_cols[0]] + epsilon)
            traffic_features_added += 1

        # Ù¾Ø±Ú†Ù…â€ŒÙ‡Ø§ÛŒ ØªØ±Ø§ÙÛŒÚ© ØºÛŒØ±Ø¹Ø§Ø¯ÛŒ
        if len(bytes_cols) >= 1:
            df['is_small_flow'] = (df[bytes_cols[0]] < 100).astype(int)
            df['is_large_flow'] = (df[bytes_cols[0]] > 1000000).astype(int)
            traffic_features_added += 2

        print(f"   âœ… {traffic_features_added} ÙˆÛŒÚ˜Ú¯ÛŒ ØªØ±Ø§ÙÛŒÚ©ÛŒ Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯")
        return df

    def _engineer_time_features(self, df):
        """Ù…Ù‡Ù†Ø¯Ø³ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø²Ù…Ø§Ù†ÛŒ - Ø¹Ù…ÙˆÙ…ÛŒ"""
        print("ğŸ”§ Ø§ÛŒØ¬Ø§Ø¯ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø²Ù…Ø§Ù†ÛŒ...")

        time_features_added = 0
        epsilon = 1e-8

        time_cols = self.detected_features['time_columns']
        timestamp_cols = self.detected_features['timestamp_columns']

        # Ù†Ø±Ø®â€ŒÙ‡Ø§ÛŒ Ù…Ø¨ØªÙ†ÛŒ Ø¨Ø± Ø²Ù…Ø§Ù†
        if time_cols:
            time_col = time_cols[0]
            bytes_cols = self.detected_features['bytes_columns']
            packets_cols = self.detected_features['packets_columns']

            if bytes_cols:
                df['bytes_per_second'] = df[bytes_cols[0]] / (df[time_col] + epsilon)
                time_features_added += 1

            if packets_cols:
                df['packets_per_second'] = df[packets_cols[0]] / (df[time_col] + epsilon)
                time_features_added += 1

            # Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ù…Ø¯Øª Ø²Ù…Ø§Ù†
            df['duration_category'] = pd.cut(
                df[time_col],
                bins=[0, 1, 30, float('inf')],
                labels=['short', 'medium', 'long'],
                right=False
            )
            time_features_added += 1

            # Ù¾Ø±Ú†Ù…â€ŒÙ‡Ø§ÛŒ Ø¯Ø³ØªÙ‡ Ø²Ù…Ø§Ù†
            df['is_short_session'] = (df[time_col] < 1).astype(int)
            df['is_long_session'] = (df[time_col] > 30).astype(int)
            time_features_added += 2

        print(f"   âœ… {time_features_added} ÙˆÛŒÚ˜Ú¯ÛŒ Ø²Ù…Ø§Ù†ÛŒ Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯")
        return df

    def _apply_log_transforms(self, df):
        """Ø§Ø¹Ù…Ø§Ù„ ØªØ¨Ø¯ÛŒÙ„ Ù„Ú¯Ø§Ø±ÛŒØªÙ…ÛŒ Ø¨Ø±Ø§ÛŒ Ú©Ø§Ù‡Ø´ skew - Ø¹Ù…ÙˆÙ…ÛŒ"""
        print("ğŸ”§ Ø§Ø¹Ù…Ø§Ù„ ØªØ¨Ø¯ÛŒÙ„ Ù„Ú¯Ø§Ø±ÛŒØªÙ…ÛŒ...")

        log_features_added = 0

        # Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ Ø¨Ø§ skew Ø¨Ø§Ù„Ø§ (Ø¨Ù‡ Ø¬Ø² Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ Ùˆ Ù¾Ø±Ú†Ù…â€ŒÙ‡Ø§)
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        exclude_patterns = ['category', 'is_', 'ratio', 'match', 'has_', 'flag']

        for col in numeric_cols:
            if any(pattern in col for pattern in exclude_patterns):
                continue

            # Ù…Ø­Ø§Ø³Ø¨Ù‡ skew Ùˆ Ø§Ø¹Ù…Ø§Ù„ log1p Ø§Ú¯Ø± skew Ø¨Ø§Ù„Ø§ Ø¨Ø§Ø´Ø¯
            skew_val = df[col].skew()
            if abs(skew_val) > 1.0:  # Ø¢Ø³ØªØ§Ù†Ù‡ skew
                df[f'log1p_{col}'] = np.log1p(df[col])
                log_features_added += 1

        print(f"   âœ… {log_features_added} ØªØ¨Ø¯ÛŒÙ„ Ù„Ú¯Ø§Ø±ÛŒØªÙ…ÛŒ Ø§Ø¹Ù…Ø§Ù„ Ø´Ø¯")
        return df

    def _encode_categorical_features(self, df, is_train=True):
        """Ø±Ù…Ø²Ú¯Ø°Ø§Ø±ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ - Ø¹Ù…ÙˆÙ…ÛŒ"""
        print("ğŸ”¤ Ø±Ù…Ø²Ú¯Ø°Ø§Ø±ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ...")

        # Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ (ØºÛŒØ±Ø¹Ø¯Ø¯ÛŒ)
        categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()

        encoded_features = 0

        # Û±. Ø§Ø¨ØªØ¯Ø§ Ø³ØªÙˆÙ† Ù‡Ø¯Ù Ø±Ø§ encode Ú©Ù†ÛŒÙ…
        if self.target_column in df.columns and df[self.target_column].dtype in ['object', 'category']:
            if is_train:
                target_encoder = LabelEncoder()
                df[self.target_column] = target_encoder.fit_transform(df[self.target_column].astype(str))
                self.encoders[self.target_column] = target_encoder
            else:
                if self.target_column in self.encoders:
                    # Ù…Ø¯ÛŒØ±ÛŒØª Ù…Ù‚Ø§Ø¯ÛŒØ± Ø¬Ø¯ÛŒØ¯ Ø¯Ø± test set
                    unique_vals = set(df[self.target_column].astype(str).unique())
                    trained_vals = set(self.encoders[self.target_column].classes_)
                    new_vals = unique_vals - trained_vals

                    if new_vals:
                        print(f"   âš ï¸  Ù…Ù‚Ø§Ø¯ÛŒØ± Ø¬Ø¯ÛŒØ¯ Ø¯Ø± Ø³ØªÙˆÙ† Ù‡Ø¯Ù: {new_vals}")
                        df[self.target_column] = df[self.target_column].astype(str)
                        df.loc[df[self.target_column].isin(new_vals), self.target_column] = 'unknown'

                    df[self.target_column] = self.encoders[self.target_column].transform(
                        df[self.target_column].astype(str))
                else:
                    raise Exception(f"Encoder Ø¨Ø±Ø§ÛŒ Ø³ØªÙˆÙ† Ù‡Ø¯Ù '{self.target_column}' ÛŒØ§ÙØª Ù†Ø´Ø¯")

            encoded_features += 1
            print(f"   âœ… {self.target_column}: Label Encoding (Ø³ØªÙˆÙ† Ù‡Ø¯Ù)")

        # Û². Ø­Ø§Ù„Ø§ Ø³Ø§ÛŒØ± Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ Ø±Ø§ encode Ú©Ù†ÛŒÙ… (Ø¨Ø¯ÙˆÙ† Ø­Ø°Ù Ø³ØªÙˆÙ† Ù‡Ø¯Ù)
        for col in categorical_cols:
            if col == self.target_column:
                continue

            if df[col].nunique() <= 10:  # One-Hot Ø¨Ø±Ø§ÛŒ Ø¯Ø³ØªÙ‡â€ŒÙ‡Ø§ÛŒ Ú©Ù…
                dummies = pd.get_dummies(df[col], prefix=col)
                df = pd.concat([df, dummies], axis=1)
                df = df.drop(columns=[col])
                encoded_features += len(dummies.columns)
                print(f"   âœ… {col}: One-Hot Encoding ({len(dummies.columns)} ÙˆÛŒÚ˜Ú¯ÛŒ)")
            else:  # Label Encoding Ø¨Ø±Ø§ÛŒ Ø¯Ø³ØªÙ‡â€ŒÙ‡Ø§ÛŒ Ø²ÛŒØ§Ø¯
                if is_train:
                    encoder = LabelEncoder()
                    df[col] = encoder.fit_transform(df[col].astype(str))
                    self.encoders[col] = encoder
                else:
                    if col in self.encoders:
                        # Ù…Ø¯ÛŒØ±ÛŒØª Ù…Ù‚Ø§Ø¯ÛŒØ± Ø¬Ø¯ÛŒØ¯ Ø¯Ø± test set
                        unique_vals = set(df[col].astype(str).unique())
                        trained_vals = set(self.encoders[col].classes_)
                        new_vals = unique_vals - trained_vals

                        if new_vals:
                            df[col] = df[col].astype(str)
                            df.loc[df[col].isin(new_vals), col] = 'unknown'

                        df[col] = self.encoders[col].transform(df[col].astype(str))
                    else:
                        # Ø§Ú¯Ø± encoder ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯ØŒ Ø­Ø°Ù Ø³ØªÙˆÙ†
                        df = df.drop(columns=[col])
                encoded_features += 1
                print(f"   âœ… {col}: Label Encoding")

        print(f"   âœ… Ø¯Ø± Ù…Ø¬Ù…ÙˆØ¹ {encoded_features} ÙˆÛŒÚ˜Ú¯ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ Ø±Ù…Ø²Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯")
        return df

    def _scale_features(self, df, fit_scalers=True):
        """Ù…Ù‚ÛŒØ§Ø³â€ŒØ¨Ù†Ø¯ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ - Ø¹Ù…ÙˆÙ…ÛŒ"""
        print("ğŸ“Š Ù…Ù‚ÛŒØ§Ø³â€ŒØ¨Ù†Ø¯ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ...")

        # Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ (Ø¨Ù‡ Ø¬Ø² Ù‡Ø¯Ù Ùˆ Ù¾Ø±Ú†Ù…â€ŒÙ‡Ø§)
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()

        # Ø­Ø°Ù Ø³ØªÙˆÙ† Ù‡Ø¯Ù Ùˆ Ù¾Ø±Ú†Ù…â€ŒÙ‡Ø§ÛŒ Ø¨Ø§ÛŒÙ†Ø±ÛŒ
        exclude_patterns = [self.target_column] + [col for col in numeric_cols
                                                   if any(x in col for x in ['is_', 'has_', 'match', 'flag'])]
        scale_cols = [col for col in numeric_cols if col not in exclude_patterns]

        if not scale_cols:
            return df

        print(f"   ğŸ“ˆ Ù…Ù‚ÛŒØ§Ø³â€ŒØ¨Ù†Ø¯ÛŒ {len(scale_cols)} ÙˆÛŒÚ˜Ú¯ÛŒ Ø¹Ø¯Ø¯ÛŒ")

        if fit_scalers:
            # Ø¢Ù…ÙˆØ²Ø´ scaler Ø¬Ø¯ÛŒØ¯
            if self.scaling_strategy == 'standard':
                scaler = StandardScaler()
            elif self.scaling_strategy == 'robust':
                scaler = RobustScaler()
            else:
                scaler = StandardScaler()

            df[scale_cols] = scaler.fit_transform(df[scale_cols])
            self.scalers['numerical'] = scaler
        else:
            # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² scaler Ø¢Ù…ÙˆØ²Ø´ Ø¯ÛŒØ¯Ù‡
            if 'numerical' in self.scalers:
                df[scale_cols] = self.scalers['numerical'].transform(df[scale_cols])
            else:
                print("âš ï¸  Ù‡ÛŒÚ† scaler Ø¢Ù…ÙˆØ²Ø´ Ø¯ÛŒØ¯Ù‡â€ŒØ§ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯")

        return df

    def _select_features(self, df, is_train=True, k_features=50):
        """Ø§Ù†ØªØ®Ø§Ø¨ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¨Ø±ØªØ± - Ø¹Ù…ÙˆÙ…ÛŒ"""
        print("ğŸ¯ Ø§Ù†ØªØ®Ø§Ø¨ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¨Ø±ØªØ±...")

        if self.target_column not in df.columns:
            return df

        X = df.drop(columns=[self.target_column])
        y = df[self.target_column]

        # Û±. Ø­Ø°Ù ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¨Ø§ ÙˆØ§Ø±ÛŒØ§Ù†Ø³ Ù¾Ø§ÛŒÛŒÙ†
        if is_train:
            variance_selector = VarianceThreshold(threshold=0.01)
            X_selected = variance_selector.fit_transform(X)
            low_var_cols = X.columns[~variance_selector.get_support()].tolist()

            if low_var_cols:
                print(f"   ğŸ—‘ï¸  Ø­Ø°Ù {len(low_var_cols)} ÙˆÛŒÚ˜Ú¯ÛŒ Ø¨Ø§ ÙˆØ§Ø±ÛŒØ§Ù†Ø³ Ù¾Ø§ÛŒÛŒÙ†")
                X = X.drop(columns=low_var_cols)

        # Û². Ø§Ù†ØªØ®Ø§Ø¨ k ÙˆÛŒÚ˜Ú¯ÛŒ Ø¨Ø±ØªØ±
        if is_train:
            k = min(k_features, X.shape[1])
            self.feature_selector = SelectKBest(score_func=f_classif, k=k)
            X_selected = self.feature_selector.fit_transform(X, y)
            selected_features = X.columns[self.feature_selector.get_support()].tolist()
        else:
            if self.feature_selector:
                X_selected = self.feature_selector.transform(X)
                selected_features = X.columns[self.feature_selector.get_support()].tolist()
            else:
                selected_features = X.columns.tolist()
                X_selected = X.values

        print(f"   âœ… Ø§Ù†ØªØ®Ø§Ø¨ {len(selected_features)} ÙˆÛŒÚ˜Ú¯ÛŒ Ø¨Ø±ØªØ±")

        # Ø§ÛŒØ¬Ø§Ø¯ DataFrame Ù†Ù‡Ø§ÛŒÛŒ
        df_selected = pd.DataFrame(X_selected, columns=selected_features, index=df.index)
        df_selected[self.target_column] = y.values

        return df_selected

    # Ø¯Ø± Ú©Ù„Ø§Ø³ SmartFeatureEngineerØŒ Ù…ØªØ¯ get_feature_summary Ø±Ø§ Ø§ØµÙ„Ø§Ø­ Ú©Ù†ÛŒØ¯:

    def get_feature_summary(self):
        """Ú¯Ø²Ø§Ø±Ø´ Ø®Ù„Ø§ØµÙ‡ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯Ù‡ """
        # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Ø§Ù†ÙˆØ§Ø¹ Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯ Ù¾Ø§ÛŒØªÙˆÙ†
        detected_features_serializable = {}
        for key, value in self.detected_features.items():
            detected_features_serializable[key] = [str(item) for item in value] if isinstance(value, list) else value

        return {
            'detected_features': detected_features_serializable,
            'fitted': self.fitted,
            'scaling_strategy': self.scaling_strategy,
            'encoders_count': int(len(self.encoders)),
            'scalers_count': int(len(self.scalers)),
            'has_feature_selector': self.feature_selector is not None
        }