from datetime import datetime
import pandas as pd
import numpy as np
import os
import sys
import json
from pathlib import Path


class DataCleaner:
    """Ú©Ù„Ø§Ø³ Ø¹Ù…ÙˆÙ…ÛŒ Ø¨Ø±Ø§ÛŒ Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù"""

    def __init__(self, data_path, output_dir=None):
        self.data_path = data_path
        self.output_dir = output_dir or os.path.dirname(data_path)
        self.cleaning_report = {}
        self.df_original = None
        self.df_cleaned = None
        # ØªØ¹Ø±ÛŒÙ Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§
        self.numeric_cols = []
        self.categorical_cols = []
        self.datetime_cols = []
        self.outlier_method = 'mark'
        self.allow_negative_cols = []
        self.illogical_values_removed = 0

    def load_data(self):
        """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡ Ùˆ Ø¨Ø±Ø±Ø³ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ - Ù†Ø³Ø®Ù‡ Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡"""
        print("=" * 50)
        print("Ú¯Ø§Ù… 1: Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡ Ùˆ Ø¨Ø±Ø±Ø³ÛŒ Ø§ÙˆÙ„ÛŒÙ‡")
        print("=" * 50)

        # ØªØ´Ø®ÛŒØµ Ø®ÙˆØ¯Ú©Ø§Ø± ÙØ±Ù…Øª ÙØ§ÛŒÙ„
        file_extension = Path(self.data_path).suffix.lower()

        try:
            if file_extension == '.csv':
                df = pd.read_csv(self.data_path, low_memory=False)
            elif file_extension in ['.xlsx', '.xls']:
                df = pd.read_excel(self.data_path)
            elif file_extension == '.parquet':
                df = pd.read_parquet(self.data_path)
            elif file_extension == '.json':
                df = pd.read_json(self.data_path)
            else:
                df = pd.read_csv(self.data_path, low_memory=False)
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ§ÛŒÙ„ {self.data_path} Ø¨Ø§ Ù¾Ø³ÙˆÙ†Ø¯ {file_extension}: {e}")
            raise

        # Ø°Ø®ÛŒØ±Ù‡ Ú©Ù¾ÛŒ Ø§Ø² Ø¯Ø§Ø¯Ù‡ Ø§ØµÙ„ÛŒ
        self.df_original = df.copy()

        print(f"ğŸ“ ÙØ§ÛŒÙ„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯: {self.data_path}")
        print(f"ğŸ“Š Ø§Ø¨Ø¹Ø§Ø¯ Ø¯Ø§Ø¯Ù‡: {df.shape} (Ø±Ø¯ÛŒÙâ€ŒÙ‡Ø§: {df.shape[0]:,}, Ø³ØªÙˆÙ†â€ŒÙ‡Ø§: {df.shape[1]})")
        print(f"ğŸ“ ÙØ±Ù…Øª ÙØ§ÛŒÙ„: {file_extension}")

        print("\nğŸ” Ù†Ù…ÙˆÙ†Ù‡â€ŒØ§ÛŒ Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§:")
        print(df.head(3))

        print("\nğŸ“‹ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ù„ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§:")
        df.info()

        # ØªØ­Ù„ÛŒÙ„ Ø§ÙˆÙ„ÛŒÙ‡ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§
        self._analyze_columns(df)

        # ØªØ´Ø®ÛŒØµ Ø®ÙˆØ¯Ú©Ø§Ø± Ø§Ù†ÙˆØ§Ø¹ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§
        self.column_analysis = self._auto_detect_column_types(df)

        # Ù†Ù…Ø§ÛŒØ´ ØªØ­Ù„ÛŒÙ„ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§
        self._print_column_analysis()

        # Ø°Ø®ÛŒØ±Ù‡ Ù„ÛŒØ³Øª Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¯Ø± Ù…ØªØ¯Ù‡Ø§ÛŒ Ø¯ÛŒÚ¯Ø±
        self.numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        self.categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()
        self.datetime_cols = df.select_dtypes(include=['datetime64']).columns.tolist()

        return df

    def _print_column_analysis(self):
        """Ú†Ø§Ù¾ ØªØ­Ù„ÛŒÙ„ Ø§Ù†ÙˆØ§Ø¹ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§"""
        print(f"\nğŸ¯ ØªØ­Ù„ÛŒÙ„ Ø®ÙˆØ¯Ú©Ø§Ø± Ø³ØªÙˆÙ†â€ŒÙ‡Ø§:")

        type_groups = {}
        for col, info in self.column_analysis.items():
            suggested_type = info['suggested_type']
            if suggested_type not in type_groups:
                type_groups[suggested_type] = []
            type_groups[suggested_type].append(col)

        for col_type, columns in type_groups.items():
            print(f"   {col_type}: {len(columns)} Ø³ØªÙˆÙ†")
            for col in columns[:5]:  # Ù†Ù…Ø§ÛŒØ´ Ûµ Ø³ØªÙˆÙ† Ø§ÙˆÙ„
                unique_info = f" ({self.column_analysis[col]['unique_count']} Ù…Ù‚Ø¯Ø§Ø± ÛŒÚ©ØªØ§)" if self.column_analysis[col][
                                                                                                  'unique_count'] < 100 else ""
                print(f"      â€¢ {col}{unique_info}")
            if len(columns) > 5:
                print(f"      â€¢ ... Ùˆ {len(columns) - 5} Ø³ØªÙˆÙ† Ø¯ÛŒÚ¯Ø±")

    def _analyze_columns(self, df):
        """ØªØ­Ù„ÛŒÙ„ Ø®ÙˆØ¯Ú©Ø§Ø± Ø§Ù†ÙˆØ§Ø¹ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§"""
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()
        datetime_cols = df.select_dtypes(include=['datetime64']).columns.tolist()

        print(f"\nğŸ¯ ØªØ­Ù„ÛŒÙ„ Ø®ÙˆØ¯Ú©Ø§Ø± Ø³ØªÙˆÙ†â€ŒÙ‡Ø§:")
        print(f"   Ø¹Ø¯Ø¯ÛŒ: {len(numeric_cols)} Ø³ØªÙˆÙ†")
        print(f"   Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ: {len(categorical_cols)} Ø³ØªÙˆÙ†")
        print(f"   ØªØ§Ø±ÛŒØ®/Ø²Ù…Ø§Ù†: {len(datetime_cols)} Ø³ØªÙˆÙ†")

        self.cleaning_report['column_analysis'] = {
            'numeric': numeric_cols,
            'categorical': categorical_cols,
            'datetime': datetime_cols
        }

    def remove_duplicates(self, df):
        """Ø­Ø°Ù Ø±Ø¯ÛŒÙâ€ŒÙ‡Ø§ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒ"""
        print("\n" + "=" * 50)
        print("Ú¯Ø§Ù… 2: Ø¨Ø±Ø±Ø³ÛŒ Ø±Ø¯ÛŒÙâ€ŒÙ‡Ø§ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒ")
        print("=" * 50)

        initial_count = len(df)
        duplicate_count = df.duplicated().sum()

        if duplicate_count > 0:
            df = df.drop_duplicates()
            removed_count = initial_count - len(df)
            print(f"âœ… {removed_count} Ø±Ø¯ÛŒÙ ØªÚ©Ø±Ø§Ø±ÛŒ Ø­Ø°Ù Ø´Ø¯.")
            self.cleaning_report['duplicates_removed'] = removed_count
        else:
            print("âœ… Ù‡ÛŒÚ† Ø±Ø¯ÛŒÙ ØªÚ©Ø±Ø§Ø±ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯.")
            self.cleaning_report['duplicates_removed'] = 0

        print(f"ğŸ“Š Ø§Ø¨Ø¹Ø§Ø¯ Ø¬Ø¯ÛŒØ¯: {df.shape}")
        return df

    def handle_missing_values(self, df, strategy='auto'):
        """Ù…Ø¯ÛŒØ±ÛŒØª Ù…Ù‚Ø§Ø¯ÛŒØ± Ú¯Ù…Ø´Ø¯Ù‡ Ø¨Ø§ Ú¯Ø²Ø§Ø±Ø´ Ø¯Ù‚ÛŒÙ‚"""
        print("\n" + "=" * 50)
        print("Ú¯Ø§Ù… 3: Ù…Ø¯ÛŒØ±ÛŒØª Ù…Ù‚Ø§Ø¯ÛŒØ± Ú¯Ù…Ø´Ø¯Ù‡")
        print("=" * 50)

        null_before = df.isnull().sum().sum()
        null_by_col_before = df.isnull().sum()

        if null_before == 0:
            print("âœ… Ù‡ÛŒÚ† Ù…Ù‚Ø¯Ø§Ø± Ú¯Ù…Ø´Ø¯Ù‡â€ŒØ§ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯.")
            self.cleaning_report['missing_values'] = {'total_removed': 0}
            return df

        print(f"ğŸ” Ù…Ù‚Ø§Ø¯ÛŒØ± Ú¯Ù…Ø´Ø¯Ù‡ Ù‚Ø¨Ù„ Ø§Ø² Ù¾Ø±Ø¯Ø§Ø²Ø´: {null_before}")

        numeric_cols = df.select_dtypes(include=[np.number]).columns
        categorical_cols = df.select_dtypes(include=['object', 'category']).columns

        fill_report = {}

        for col in df.columns:
            null_count = df[col].isnull().sum()
            if null_count == 0:
                continue

            null_percentage = (null_count / len(df)) * 100
            print(f"\nğŸ“Š {col}: {null_count} Ù…Ù‚Ø¯Ø§Ø± Ú¯Ù…Ø´Ø¯Ù‡ ({null_percentage:.1f}%)")

            if null_percentage > 50:  # Ø­Ø°Ù Ø³ØªÙˆÙ† Ø§Ú¯Ø± Ø¨ÛŒØ´ Ø§Ø² ÛµÛ°Ùª Ù…Ù‚Ø§Ø¯ÛŒØ± Ú¯Ù…Ø´Ø¯Ù‡
                df = df.drop(columns=[col])
                fill_report[col] = {'action': 'column_dropped', 'reason': 'high_missing_rate'}
                print(f"   ğŸ—‘ï¸  Ø³ØªÙˆÙ† Ø­Ø°Ù Ø´Ø¯ (Ù†Ø±Ø® Ú¯Ù…Ø´Ø¯Ù‡ Ø¨Ø§Ù„Ø§)")

            elif null_percentage > 15:  # Ø­Ø°Ù Ø±Ø¯ÛŒÙâ€ŒÙ‡Ø§ Ø§Ú¯Ø± Û±Ûµ-ÛµÛ°Ùª Ú¯Ù…Ø´Ø¯Ù‡
                df = df.dropna(subset=[col])
                fill_report[col] = {'action': 'rows_dropped', 'count': null_count}
                print(f"   ğŸ“ {null_count} Ø±Ø¯ÛŒÙ Ø­Ø°Ù Ø´Ø¯")

            else:  # Ù¾Ø± Ú©Ø±Ø¯Ù† Ù…Ù‚Ø§Ø¯ÛŒØ±
                if col in numeric_cols:
                    fill_value = df[col].median()
                    df[col] = df[col].fillna(fill_value)
                    fill_report[col] = {'action': 'filled', 'value': fill_value, 'method': 'median'}
                    print(f"   ğŸ”¢ Ù¾Ø± Ø´Ø¯Ù‡ Ø¨Ø§ Ù…ÛŒØ§Ù†Ù‡: {fill_value}")
                elif col in categorical_cols:
                    fill_value = df[col].mode()[0] if len(df[col].mode()) > 0 else 'Unknown'
                    df[col] = df[col].fillna(fill_value)
                    fill_report[col] = {'action': 'filled', 'value': fill_value, 'method': 'mode'}
                    print(f"   ğŸ“ Ù¾Ø± Ø´Ø¯Ù‡ Ø¨Ø§ Ù…Ø¯: {fill_value}")
                else:
                    df[col] = df[col].fillna('Unknown')
                    fill_report[col] = {'action': 'filled', 'value': 'Unknown', 'method': 'constant'}
                    print(f"   ğŸ”¤ Ù¾Ø± Ø´Ø¯Ù‡ Ø¨Ø§: Unknown")

        null_after = df.isnull().sum().sum()
        self.cleaning_report['missing_values'] = {
            'total_before': null_before,
            'total_after': null_after,
            'removed': null_before - null_after,
            'details': fill_report
        }

        print(f"\nâœ… Ù…Ù‚Ø§Ø¯ÛŒØ± Ú¯Ù…Ø´Ø¯Ù‡ Ù¾Ø³ Ø§Ø² Ù¾Ø±Ø¯Ø§Ø²Ø´: {null_after}")
        return df

    def optimize_data_types(self, df):
        """Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø®ÙˆØ¯Ú©Ø§Ø± Ù†ÙˆØ¹ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ """
        print("\n" + "=" * 50)
        print("Ú¯Ø§Ù… 5: Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù†ÙˆØ¹ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§")
        print("=" * 50)

        conversion_report = {}

        for col, col_info in self.column_analysis.items():
            if col not in df.columns:
                continue

            original_dtype = str(df[col].dtype)
            suggested_type = col_info['suggested_type']

            try:
                if suggested_type == 'category' and not pd.api.types.is_categorical_dtype(df[col]):
                    df[col] = df[col].astype('category')
                    conversion_report[col] = {
                        'from': original_dtype,
                        'to': 'category',
                        'reason': 'low_cardinality'
                    }

                elif suggested_type == 'numeric' and pd.api.types.is_numeric_dtype(df[col]):
                    # Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù†ÙˆØ¹ Ø¹Ø¯Ø¯ÛŒ
                    if df[col].min() >= 0:
                        if df[col].max() < 255:
                            new_dtype = 'uint8'
                        elif df[col].max() < 65535:
                            new_dtype = 'uint16'
                        else:
                            new_dtype = 'uint32'
                    else:
                        if df[col].min() >= -128 and df[col].max() < 127:
                            new_dtype = 'int8'
                        elif df[col].min() >= -32768 and df[col].max() < 32767:
                            new_dtype = 'int16'
                        else:
                            new_dtype = 'int32'

                    df[col] = df[col].astype(new_dtype)
                    conversion_report[col] = {
                        'from': original_dtype,
                        'to': new_dtype,
                        'reason': 'numeric_optimization'
                    }

                elif suggested_type == 'datetime' and not pd.api.types.is_datetime64_any_dtype(df[col]):
                    try:
                        df[col] = pd.to_datetime(df[col], errors='coerce')
                        conversion_report[col] = {
                            'from': original_dtype,
                            'to': 'datetime64[ns]',
                            'reason': 'temporal_conversion'
                        }
                    except:
                        pass

            except Exception as e:
                print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø³ØªÙˆÙ† {col}: {e}")

        # Ù†Ù…Ø§ÛŒØ´ ØªØºÛŒÛŒØ±Ø§Øª
        if conversion_report:
            for col, info in conversion_report.items():
                print(f"âœ… {col}: {info['from']} â†’ {info['to']} ({info['reason']})")
        else:
            print("âœ… Ù‡Ù…Ù‡ Ù†ÙˆØ¹ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ù‡ÛŒÙ†Ù‡ Ù‡Ø³ØªÙ†Ø¯")

        self.cleaning_report['type_conversions'] = conversion_report
        return df

    def validate_data_quality(self, df, allow_negative_cols=None):
        """Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ù†Ù‡Ø§ÛŒÛŒ Ú©ÛŒÙÛŒØª Ø¯Ø§Ø¯Ù‡"""
        print("\n" + "=" * 50)
        print("Ú¯Ø§Ù… 6: Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ú©ÛŒÙÛŒØª Ø¯Ø§Ø¯Ù‡")
        print("=" * 50)

        if allow_negative_cols is None:
            allow_negative_cols = []

        # Ø¨Ø±Ø±Ø³ÛŒ Ù…Ù‚Ø§Ø¯ÛŒØ± Ù…Ù†ÙÛŒ ÙÙ‚Ø· Ø¨Ø±Ø§ÛŒ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ Ú©Ù‡ Ø§Ø¬Ø§Ø²Ù‡ Ù…Ù†ÙÛŒ Ù†Ø¯Ø§Ø±Ù†Ø¯
        numeric_cols_to_check = [col for col in self.numeric_cols if col not in allow_negative_cols]

        has_negative_numeric = False
        if numeric_cols_to_check:
            negative_check = df[numeric_cols_to_check] < 0
            has_negative_numeric = negative_check.any().any()

        validation_results = {
            'has_duplicates': df.duplicated().sum() == 0,
            'has_null_values': df.isnull().sum().sum() == 0,
            'data_types_optimized': len(self.cleaning_report.get('type_conversions', {})) > 0,
            'has_negative_numeric': has_negative_numeric,
        }

        # Ú¯Ø²Ø§Ø±Ø´ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ
        checks = [
            ("Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯ ØªÚ©Ø±Ø§Ø±ÛŒâ€ŒÙ‡Ø§", validation_results['has_duplicates'], "âŒ", "âœ…"),
            ("Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯ Ù…Ù‚Ø§Ø¯ÛŒØ± Null", validation_results['has_null_values'], "âŒ", "âœ…"),
            ("Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯ Ù…Ù‚Ø§Ø¯ÛŒØ± Ù…Ù†ÙÛŒ ØºÛŒØ±Ù…Ù†Ø·Ù‚ÛŒ", not validation_results['has_negative_numeric'], "âš ï¸", "âœ…"),
            ("Ù†ÙˆØ¹ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø´Ø¯Ù‡", validation_results['data_types_optimized'], "â„¹ï¸", "âœ…")
        ]

        for check_name, result, fail_icon, pass_icon in checks:
            icon = pass_icon if result else fail_icon
            print(f"{icon} {check_name}")

        all_passed = all([
            validation_results['has_duplicates'],
            validation_results['has_null_values'],
            not validation_results['has_negative_numeric']
        ])

        validation_results['all_passed'] = all_passed
        self.cleaning_report['validation'] = validation_results

        return all_passed

    def save_cleaned_data(self, df, output_path=None):
        """Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø§Ø¯Ù‡ Ù¾Ø§Ú©â€ŒØ´Ø¯Ù‡ Ùˆ Ú¯Ø²Ø§Ø±Ø´ """
        print("\n" + "=" * 50)
        print("Ú¯Ø§Ù… 7: Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬")
        print("=" * 50)

        if output_path is None:
            input_name = Path(self.data_path).stem
            output_path = os.path.join(self.output_dir, f"{input_name}_cleaned.csv")

        # Ø§ÛŒØ¬Ø§Ø¯ Ù¾ÙˆØ´Ù‡ Ø®Ø±ÙˆØ¬ÛŒ Ø§Ú¯Ø± ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯
        os.makedirs(self.output_dir, exist_ok=True)

        # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø§Ø¯Ù‡ Ù¾Ø§Ú©â€ŒØ´Ø¯Ù‡
        df.to_csv(output_path, index=False)
        print(f"ğŸ’¾ Ø¯Ø§Ø¯Ù‡ Ù¾Ø§Ú©â€ŒØ´Ø¯Ù‡ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {output_path}")

        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¢Ù…Ø§Ø± Ø­Ø§ÙØ¸Ù‡
        original_memory = self.df_original.memory_usage(deep=True).sum() if self.df_original is not None else 0
        cleaned_memory = df.memory_usage(deep=True).sum()
        memory_saved = original_memory - cleaned_memory
        memory_reduction_pct = (memory_saved / original_memory * 100) if original_memory > 0 else 0

        # Ø³Ø§Ø®Øª Ú¯Ø²Ø§Ø±Ø´ Ø¨Ø§ Ø³Ø§Ø®ØªØ§Ø± Ø³Ù„Ø³Ù„Ù‡â€ŒÙ…Ø±Ø§ØªØ¨ÛŒ
        comprehensive_report = {
            # ==================== METADATA ====================
            "metadata": {
                "project": "Network Activity Classification Pipeline",
                "phase": "Data Cleaning - Phase 1",
                "timestamp": datetime.now().isoformat(),
                "version": "1.0",
                "input_file": str(self.data_path),
                "output_file": str(output_path),
                "cleaning_parameters": {
                    "outlier_method": getattr(self, 'outlier_method', 'mark'),
                    "allow_negative_cols": getattr(self, 'allow_negative_cols', [])
                },
                "execution_environment": {
                    "python_version": sys.version,
                    "pandas_version": pd.__version__,
                    "numpy_version": np.__version__
                }
            },

            # ==================== EXECUTION SUMMARY ====================
            "execution_summary": {
                "status": "completed_successfully",
                "original_dataset": {
                    "rows": int(self.df_original.shape[0]) if self.df_original is not None else 0,
                    "columns": int(self.df_original.shape[1]) if self.df_original is not None else 0,
                    "memory_bytes": int(original_memory)
                },
                "cleaned_dataset": {
                    "rows": int(df.shape[0]),
                    "columns": int(df.shape[1]),
                    "memory_bytes": int(cleaned_memory)
                },
                "reduction_metrics": {
                    "rows_removed": int(
                        (self.df_original.shape[0] if self.df_original is not None else 0) - df.shape[0]),
                    "rows_removed_percentage": float((((self.df_original.shape[
                                                            0] if self.df_original is not None else 0) - df.shape[
                                                           0]) / (self.df_original.shape[
                                                                      0] if self.df_original is not None else 1)) * 100),
                    "memory_saved_bytes": int(memory_saved),
                    "memory_reduction_percentage": float(memory_reduction_pct)
                }
            },

            # ==================== DATA QUALITY ASSESSMENT ====================
            "data_quality_metrics": {
                "completeness_score": self._calculate_completeness_score(df),
                "consistency_score": self._calculate_consistency_score(df),
                "validity_score": self._calculate_validity_score(df),
                "uniqueness_score": self._calculate_uniqueness_score(df),
                "overall_quality_score": self._calculate_overall_quality_score(df),
                "quality_grades": {
                    "completeness": self._get_quality_grade(self._calculate_completeness_score(df)),
                    "consistency": self._get_quality_grade(self._calculate_consistency_score(df)),
                    "validity": self._get_quality_grade(self._calculate_validity_score(df)),
                    "overall": self._get_quality_grade(self._calculate_overall_quality_score(df))
                }
            },

            # ==================== CLEANING OPERATIONS DETAILS ====================
            "cleaning_operations": {
                "duplicate_removal": {
                    "rows_removed": self.cleaning_report.get('duplicates_removed', 0),
                    "remaining_duplicates": int(df.duplicated().sum()),
                    "effectiveness": "complete" if df.duplicated().sum() == 0 else "partial"
                },
                "missing_values_handling": self._enhance_missing_values_report(),
                "outlier_management": self._enhance_outlier_report(df),
                "data_type_optimization": {
                    "columns_optimized": len(self.cleaning_report.get('type_conversions', {})),
                    "conversions": self.cleaning_report.get('type_conversions', {}),
                    "memory_impact_bytes": int(memory_saved)
                },
                "invalid_data_removal": {
                    "rows_removed": getattr(self, 'illogical_values_removed', 0),
                    "types_found": ["negative_values", "out_of_range_ports"]
                }
            },

            # ==================== COLUMN-WISE ANALYSIS ====================
            "column_analysis": {
                "summary": {
                    "total_columns": len(df.columns),
                    "numeric_columns": len(self.numeric_cols),
                    "categorical_columns": len(self.categorical_cols),
                    "datetime_columns": len(self.datetime_cols),
                    "outlier_columns": len([col for col in df.columns if col.endswith('_outlier')])
                },
                "per_column_details": self._generate_detailed_column_analysis(df)
            },

            # ==================== STATISTICAL SUMMARY ====================
            "statistical_summary": {
                "dataset_statistics": {
                    "total_records": int(len(df)),
                    "total_columns": int(len(df.columns)),
                    "total_memory_bytes": int(cleaned_memory),
                    "average_record_size_bytes": int(cleaned_memory / len(df)) if len(df) > 0 else 0
                },
                "data_type_distribution": {
                    str(dtype): int(len(df.select_dtypes(include=[dtype]).columns))
                    for dtype in df.dtypes.unique()
                },
                "quality_indicators": {
                    "null_free_columns": int(len([col for col in df.columns if df[col].isnull().sum() == 0])),
                    "constant_columns": int(len([col for col in df.columns if df[col].nunique() <= 1])),
                    "high_cardinality_columns": int(
                        len([col for col in df.columns if df[col].nunique() > len(df) * 0.5]))
                }
            }
        }

        # Ø°Ø®ÛŒØ±Ù‡ Ú¯Ø²Ø§Ø±Ø´
        report_path = os.path.join(self.output_dir, "cleaning_report.json")

        try:
            with open(report_path, 'w', encoding='utf-8') as f:
                json.dump(comprehensive_report, f, indent=2, ensure_ascii=False, default=self._json_serializer)
            columns_metadata = {
                "numeric_columns": self.numeric_cols,
                "categorical_columns": self.categorical_cols,
                "datetime_columns": self.datetime_cols,
                "all_columns": df.columns.tolist(),
                "column_dtypes": {col: str(dtype) for col, dtype in df.dtypes.items()}
            }

            columns_metadata_path = os.path.join(self.output_dir, "../columns_metadata.json")
            with open(columns_metadata_path, "w", encoding="utf-8") as f:
                json.dump(columns_metadata, f, indent=2, ensure_ascii=False)

            print(f"ğŸ§© Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {columns_metadata_path}")
            print(f"ğŸ“Š Ú¯Ø²Ø§Ø±Ø´ Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {report_path}")

            # Ù†Ù…Ø§ÛŒØ´ Ø®Ù„Ø§ØµÙ‡â€ŒØ§ÛŒ Ø§Ø² Ú¯Ø²Ø§Ø±Ø´
            self._print_comprehensive_summary(comprehensive_report, df)

        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú¯Ø²Ø§Ø±Ø´: {e}")
            # Fallback: Ø°Ø®ÛŒØ±Ù‡ Ú¯Ø²Ø§Ø±Ø´ Ø³Ø§Ø¯Ù‡â€ŒØªØ±
            self._save_fallback_report(report_path, comprehensive_report)

        return output_path, report_path

    def _json_serializer(self, obj):
        """ØªØ§Ø¨Ø¹ Ø³Ø±ÛŒØ§Ù„Ø§ÛŒØ²Ø± Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ JSON"""
        if isinstance(obj, (np.integer, np.int64, np.int32, np.int16, np.int8)):
            return int(obj)
        elif isinstance(obj, (np.floating, np.float64, np.float32, np.float16)):
            return float(obj)
        elif isinstance(obj, np.bool_):
            return bool(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif hasattr(obj, 'isoformat'):  # Ø¨Ø±Ø§ÛŒ datetime
            return obj.isoformat()
        elif isinstance(obj, pd.Timestamp):
            return obj.isoformat()
        elif isinstance(obj, pd.Categorical):
            return obj.tolist()
        else:
            raise TypeError(f"Object of type {type(obj)} is not JSON serializable")

    def _calculate_completeness_score(self, df):
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù†Ù…Ø±Ù‡ Ú©Ø§Ù…Ù„ Ø¨ÙˆØ¯Ù† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§"""
        total_cells = df.shape[0] * df.shape[1]
        missing_cells = df.isnull().sum().sum()
        completeness = (total_cells - missing_cells) / total_cells if total_cells > 0 else 1.0
        return float(completeness * 100)

    def _calculate_consistency_score(self, df):
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù†Ù…Ø±Ù‡ Ø³Ø§Ø²Ú¯Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ """
        try:
            consistency_checks = []

            # Ø¨Ø±Ø±Ø³ÛŒ Bytes consistency
            if all(col in df.columns for col in ['Bytes', 'Bytes Sent', 'Bytes Received']):
                # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² np.isclose Ø¨Ø±Ø§ÛŒ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø§Ø¹Ø¯Ø§Ø¯ Ø§Ø¹Ø´Ø§Ø±ÛŒ
                bytes_consistent = np.isclose(
                    df['Bytes'],
                    df['Bytes Sent'] + df['Bytes Received'],
                    rtol=1e-5
                ).mean()
                consistency_checks.append(float(bytes_consistent))

            # Ø¨Ø±Ø±Ø³ÛŒ Packets consistency
            if all(col in df.columns for col in ['Packets', 'pkts_sent', 'pkts_received']):
                packets_consistent = np.isclose(
                    df['Packets'],
                    df['pkts_sent'] + df['pkts_received'],
                    rtol=1e-5
                ).mean()
                consistency_checks.append(float(packets_consistent))

            return float(np.mean(consistency_checks) * 100) if consistency_checks else 100.0

        except Exception as e:
            print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø³Ø§Ø²Ú¯Ø§Ø±ÛŒ: {e}")
            return 100.0  # Ø¯Ø± ØµÙˆØ±Øª Ø®Ø·Ø§

    def _calculate_validity_score(self, df):
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù†Ù…Ø±Ù‡ Ø§Ø¹ØªØ¨Ø§Ø± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ """
        try:
            valid_cells = 0
            total_cells = df.shape[0] * df.shape[1]

            for col in df.columns:
                if pd.api.types.is_numeric_dtype(df[col]):
                    # Ø¨Ø±Ø±Ø³ÛŒ Ù…Ù‚Ø§Ø¯ÛŒØ± Ù…Ù†ÙÛŒ ÙÙ‚Ø· Ø¨Ø±Ø§ÛŒ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ù†Ø¨Ø§ÛŒØ¯ Ù…Ù†ÙÛŒ Ø¨Ø§Ø´Ù†Ø¯
                    if col not in getattr(self, 'allow_negative_cols', []):
                        valid_cells += (df[col] >= 0).sum()
                    else:
                        valid_cells += len(df)  # Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù…Ø¬Ø§Ø² Ø¨Ø±Ø§ÛŒ Ù…Ù†ÙÛŒ
                else:
                    valid_cells += len(df)  # Ø¨Ø±Ø§ÛŒ Ø³Ø§ÛŒØ± Ø§Ù†ÙˆØ§Ø¹

            return float((valid_cells / total_cells) * 100) if total_cells > 0 else 100.0
        except Exception as e:
            print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ø¹ØªØ¨Ø§Ø±: {e}")
            return 100.0

    def _calculate_uniqueness_score(self, df):
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù†Ù…Ø±Ù‡ ÛŒÚ©ØªØ§ÛŒÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§"""
        duplicate_ratio = df.duplicated().sum() / len(df) if len(df) > 0 else 0
        return float((1 - duplicate_ratio) * 100)

    def _calculate_overall_quality_score(self, df):
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù†Ù…Ø±Ù‡ Ú©Ù„ÛŒ Ú©ÛŒÙÛŒØª Ø¯Ø§Ø¯Ù‡"""
        scores = [
            self._calculate_completeness_score(df) * 0.3,
            self._calculate_consistency_score(df) * 0.25,
            self._calculate_validity_score(df) * 0.25,
            self._calculate_uniqueness_score(df) * 0.2
        ]
        return float(sum(scores))

    def _get_quality_grade(self, score):
        """Ø¯Ø±Ø¬Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ú©ÛŒÙÛŒØª Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†Ù…Ø±Ù‡"""
        if score >= 95:
            return "A+"
        elif score >= 90:
            return "A"
        elif score >= 85:
            return "B+"
        elif score >= 80:
            return "B"
        elif score >= 70:
            return "C"
        else:
            return "D"

    def _enhance_missing_values_report(self):
        """Ú¯Ø²Ø§Ø±Ø´ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ù…Ø¯ÛŒØ±ÛŒØª Ù…Ù‚Ø§Ø¯ÛŒØ± Ú¯Ù…Ø´Ø¯Ù‡"""
        missing_report = self.cleaning_report.get('missing_values', {})

        enhanced_report = {
            "total_missing_before": missing_report.get('total_before', 0),
            "total_missing_after": missing_report.get('total_after', 0),
            "missing_resolution_rate": float(
                (missing_report.get('total_before', 0) - missing_report.get('total_after', 0)) /
                missing_report.get('total_before', 1) * 100
            ) if missing_report.get('total_before', 0) > 0 else 100.0,
            "handling_strategy": "auto_imputation",
            "details": missing_report.get('details', {})
        }

        return enhanced_report

    def _enhance_outlier_report(self, df):
        """Ú¯Ø²Ø§Ø±Ø´ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ù…Ø¯ÛŒØ±ÛŒØª outliers"""
        outlier_report = self.cleaning_report.get('outliers', {})

        total_outliers = sum(stats.get('count', 0) for stats in outlier_report.values())
        total_cells = df.shape[0] * len([col for col in outlier_report.keys() if not col.endswith('_outlier')])

        enhanced_report = {
            "total_columns_analyzed": len(outlier_report),
            "total_outliers_detected": total_outliers,
            "outlier_percentage": float((total_outliers / total_cells) * 100) if total_cells > 0 else 0,
            "outlier_management_strategy": getattr(self, 'outlier_method', 'mark'),
            "columns_with_outliers": [
                {
                    "column": col,
                    "outlier_count": stats.get('count', 0),
                    "outlier_percentage": stats.get('percentage', 0),
                    "normal_range": [stats.get('lower_bound', 0), stats.get('upper_bound', 0)],
                    "actual_range": [stats.get('min', 0), stats.get('max', 0)]
                }
                for col, stats in outlier_report.items() if not col.endswith('_outlier')
            ]
        }

        return enhanced_report

    def _generate_detailed_column_analysis(self, df):
        """ØªÙˆÙ„ÛŒØ¯ ØªØ­Ù„ÛŒÙ„ Ø¯Ù‚ÛŒÙ‚ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø³ØªÙˆÙ†"""
        column_details = {}

        for col in df.columns:
            col_info = {
                "data_type": str(df[col].dtype),
                "total_values": int(len(df)),
                "missing_values": {
                    "count": int(df[col].isnull().sum()),
                    "percentage": float((df[col].isnull().sum() / len(df)) * 100)
                },
                "unique_values": {
                    "count": int(df[col].nunique()),
                    "percentage": float((df[col].nunique() / len(df)) * 100)
                },
                "data_category": self._get_data_category(df[col])
            }

            # Ø¢Ù…Ø§Ø±Ù‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ
            if pd.api.types.is_numeric_dtype(df[col]):
                col_info["numerical_statistics"] = {
                    "min": float(df[col].min()),
                    "max": float(df[col].max()),
                    "mean": float(df[col].mean()),
                    "median": float(df[col].median()),
                    "std": float(df[col].std()),
                    "q1": float(df[col].quantile(0.25)),
                    "q3": float(df[col].quantile(0.75))
                }

            # Ø¢Ù…Ø§Ø±Ù‡Ø§ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ
            elif pd.api.types.is_string_dtype(df[col]) or pd.api.types.is_categorical_dtype(df[col]):
                value_counts = df[col].value_counts()
                col_info["categorical_statistics"] = {
                    "top_values": {
                        str(value): int(count) for value, count in value_counts.head(10).items()
                    },
                    "value_distribution": {
                        "most_frequent": str(value_counts.index[0]) if len(value_counts) > 0 else None,
                        "most_frequent_count": int(value_counts.iloc[0]) if len(value_counts) > 0 else 0,
                        "most_frequent_percentage": float((value_counts.iloc[0] / len(df)) * 100) if len(
                            value_counts) > 0 else 0
                    }
                }

            column_details[col] = col_info

        return column_details

    def _get_data_category(self, series):
        """ØªØ¹ÛŒÛŒÙ† Ø¯Ø³ØªÙ‡ Ø¯Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø³ØªÙˆÙ†"""
        if pd.api.types.is_numeric_dtype(series):
            return "numerical"
        elif pd.api.types.is_string_dtype(series):
            return "text"
        elif pd.api.types.is_categorical_dtype(series):
            return "categorical"
        elif pd.api.types.is_datetime64_any_dtype(series):
            return "datetime"
        else:
            return "unknown"

    def _print_comprehensive_summary(self, report, df):
        """Ú†Ø§Ù¾ Ø®Ù„Ø§ØµÙ‡ Ø¬Ø§Ù…Ø¹ Ø§Ø² Ú¯Ø²Ø§Ø±Ø´"""
        print(f"\nğŸ“‹ Ø®Ù„Ø§ØµÙ‡ Ø¬Ø§Ù…Ø¹ Ú¯Ø²Ø§Ø±Ø´ Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ:")
        print(
            f"   ğŸ“Š Ø§Ø¨Ø¹Ø§Ø¯ Ø¯Ø§Ø¯Ù‡: {report['execution_summary']['cleaned_dataset']['rows']:,} Ø³Ø·Ø± Ã— {report['execution_summary']['cleaned_dataset']['columns']} Ø³ØªÙˆÙ†")
        print(
            f"   ğŸ’¾ ØµØ±ÙÙ‡â€ŒØ¬ÙˆÛŒÛŒ Ø­Ø§ÙØ¸Ù‡: {report['execution_summary']['reduction_metrics']['memory_saved_bytes']:,} Ø¨Ø§ÛŒØª ({report['execution_summary']['reduction_metrics']['memory_reduction_percentage']:.1f}%)")
        print(
            f"   ğŸ§¹ Ø³Ø·Ø±Ù‡Ø§ÛŒ Ø­Ø°Ù Ø´Ø¯Ù‡: {report['execution_summary']['reduction_metrics']['rows_removed']:,} ({report['execution_summary']['reduction_metrics']['rows_removed_percentage']:.1f}%)")
        print(
            f"   ğŸ“ˆ Ù†Ù…Ø±Ù‡ Ú©ÛŒÙÛŒØª Ø¯Ø§Ø¯Ù‡: {report['data_quality_metrics']['overall_quality_score']:.1f}% ({report['data_quality_metrics']['quality_grades']['overall']})")
        print(
            f"   ğŸ¯ Outliers Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯Ù‡: {report['cleaning_operations']['outlier_management']['total_outliers_detected']:,}")
        print(
            f"   ğŸ”§ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ´Ø¯Ù‡: {report['cleaning_operations']['data_type_optimization']['columns_optimized']}")

    def _save_fallback_report(self, report_path, comprehensive_report):
        """Ø°Ø®ÛŒØ±Ù‡ Ú¯Ø²Ø§Ø±Ø´ Ø¯Ø± ØµÙˆØ±Øª Ø¨Ø±ÙˆØ² Ø®Ø·Ø§"""
        fallback_report = {
            "metadata": comprehensive_report["metadata"],
            "execution_summary": comprehensive_report["execution_summary"],
            "status": "completed_with_warnings",
            "error": "Simplified report due to serialization issues"
        }

        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(fallback_report, f, indent=2, ensure_ascii=False)
        print(f"ğŸ“Š Ú¯Ø²Ø§Ø±Ø´ Ø³Ø§Ø¯Ù‡ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {report_path}")

    def detect_and_handle_outliers(self, df, method='remove'):
        """Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ùˆ Ù…Ø¯ÛŒØ±ÛŒØª outlierÙ‡Ø§ Ø¨Ø§ Ù‚Ø§Ø¨Ù„ÛŒØª Ø­Ø°Ù"""
        print("\n" + "=" * 50)
        print("Ú¯Ø§Ù… 4: Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ùˆ Ù…Ø¯ÛŒØ±ÛŒØª Outliers")
        print("=" * 50)

        numeric_cols = df.select_dtypes(include=[np.number]).columns
        outlier_report = {}

        initial_row_count = len(df)
        outlier_flags = pd.Series([False] * len(df), index=df.index)

        # Ù…Ø­Ø§Ø³Ø¨Ù‡ ÙØ±Ø§ÙˆØ§Ù†ÛŒ Ù‡Ø± Ø¨Ø±Ú†Ø³Ø¨ Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ÛŒ Ú©Ù…â€ŒØ¯Ø§Ø¯Ù‡
        target_col = "Action" if "Action" in df.columns else None
        rare_classes = set()
        if target_col:
            class_counts = df[target_col].value_counts()
            rare_classes = set(class_counts[class_counts < 100].index)  # <100 Ù†Ù…ÙˆÙ†Ù‡ ÛŒØ¹Ù†ÛŒ Ú©Ù„Ø§Ø³ Ú©Ù…â€ŒØªØ¹Ø¯Ø§Ø¯
            if rare_classes:
                print(f"ğŸ” Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ÛŒ Ú©Ù…â€ŒØªØ¹Ø¯Ø§Ø¯ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒâ€ŒØ´Ø¯Ù‡ Ùˆ Ù…Ø­Ø§ÙØ¸Øªâ€ŒØ´Ø¯Ù‡ Ø¯Ø± Ø­Ø°Ù OutlierÙ‡Ø§: {rare_classes}")

        for col in numeric_cols:
            try:
                Q1, Q3 = df[col].quantile([0.25, 0.75])
                IQR = Q3 - Q1
                lower, upper = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR

                outliers_mask = (df[col] < lower) | (df[col] > upper)
                outlier_count = outliers_mask.sum()
                outlier_percentage = (outlier_count / len(df)) * 100

                outlier_report[col] = {
                    'count': outlier_count,
                    'percentage': outlier_percentage,
                    'lower_bound': lower,
                    'upper_bound': upper,
                    'min': df[col].min(),
                    'max': df[col].max()
                }

                print(f"\nğŸ“Š {col}:")
                print(f"   ğŸ“ˆ Ù…Ø­Ø¯ÙˆØ¯Ù‡ Ù…Ø¹Ù…ÙˆÙ„: [{lower:.2f}, {upper:.2f}]")
                print(f"   ğŸ“Š Ù…Ù‚Ø§Ø¯ÛŒØ± ÙˆØ§Ù‚Ø¹ÛŒ: [{df[col].min():.2f}, {df[col].max():.2f}]")
                print(f"   âš ï¸  ØªØ¹Ø¯Ø§Ø¯ outliers: {outlier_count} ({outlier_percentage:.1f}%)")

                outlier_flags = outlier_flags | outliers_mask

            except Exception as e:
                print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ outliers Ø¨Ø±Ø§ÛŒ Ø³ØªÙˆÙ† {col}: {e}")

        # ğŸš« Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø­Ø°Ù Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ú©Ù…â€ŒØªØ¹Ø¯Ø§Ø¯
        if method == 'remove' and outlier_flags.any():
            protected_mask = pd.Series(False, index=df.index)
            if target_col and rare_classes:
                protected_mask = df[target_col].isin(rare_classes)
            combined_mask = outlier_flags & ~protected_mask

            df_clean = df[~combined_mask]
            removed_count = len(df) - len(df_clean)
            protected_count = (outlier_flags & protected_mask).sum()

            print(f"\nğŸ—‘ï¸  Ø­Ø°Ù {removed_count} Ø±Ø¯ÛŒÙ outlier ({removed_count / len(df) * 100:.1f}%)")
            if protected_count:
                print(f"ğŸ›¡ï¸  Ø­ÙØ¸ {protected_count} outlier Ø§Ø² Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ÛŒ Ú©Ù…â€ŒØªØ¹Ø¯Ø§Ø¯ ({rare_classes})")

            self.cleaning_report['outliers_removed'] = removed_count
            df = df_clean

        self.cleaning_report['outliers'] = outlier_report
        return df

    def clean(self, outlier_method='remove', allow_negative_cols=None):
        """Ø§Ø¬Ø±Ø§ÛŒ Ú©Ø§Ù…Ù„ ÙØ±Ø¢ÛŒÙ†Ø¯ Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ"""
        print("ğŸš€ Ø´Ø±ÙˆØ¹ ÙØ±Ø¢ÛŒÙ†Ø¯ Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§")
        print("=" * 60)

        try:
            # Ø°Ø®ÛŒØ±Ù‡ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§
            self.outlier_method = outlier_method
            self.allow_negative_cols = allow_negative_cols or []

            # Ø§Ø¬Ø±Ø§ÛŒ Ù…Ø±Ø§Ø­Ù„ Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ
            df = self.load_data()
            df = self.remove_duplicates(df)
            df = self.handle_missing_values(df)
            df = self.detect_and_handle_outliers(df, method=outlier_method)
            df = self.optimize_data_types(df)

            # Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ù†Ù‡Ø§ÛŒÛŒ
            is_valid = self.validate_data_quality(df, allow_negative_cols=allow_negative_cols)
            self.df_cleaned = df

            # Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬
            data_path, report_path = self.save_cleaned_data(df)

            return {
                'success': True,
                'cleaned_data': df,
                'data_path': data_path,
                'report_path': report_path,
                'is_valid': is_valid,
                'report': self.cleaning_report
            }

        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ: {e}")
            return {
                'success': False,
                'error': str(e)
            }

    def _auto_detect_column_types(self, df):
        """ØªØ´Ø®ÛŒØµ Ø®ÙˆØ¯Ú©Ø§Ø± Ø§Ù†ÙˆØ§Ø¹ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†Ø§Ù… Ùˆ Ù…Ø­ØªÙˆØ§"""
        column_analysis = {}

        for col in df.columns:
            col_info = {
                'original_dtype': str(df[col].dtype),
                'unique_count': df[col].nunique(),
                'null_count': df[col].isnull().sum(),
                'suggested_type': None,
                'is_identifier': False,
                'is_temporal': False,
                'is_categorical': False
            }

            # ØªØ´Ø®ÛŒØµ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†Ø§Ù… Ø³ØªÙˆÙ†
            col_lower = col.lower()

            if any(keyword in col_lower for keyword in ['id', 'key', 'code', 'index']):
                col_info['is_identifier'] = True
                col_info['suggested_type'] = 'identifier'

            elif any(keyword in col_lower for keyword in ['time', 'date', 'year', 'month', 'day']):
                col_info['is_temporal'] = True
                if pd.api.types.is_string_dtype(df[col]):
                    # Ø³Ø¹ÛŒ Ø¯Ø± ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ datetime
                    try:
                        pd.to_datetime(df[col], errors='coerce')
                        col_info['suggested_type'] = 'datetime'
                    except:
                        col_info['suggested_type'] = 'string'

            elif any(keyword in col_lower for keyword in ['port', 'protocol', 'type', 'status', 'action']):
                col_info['is_categorical'] = True
                if df[col].nunique() < 50:  # ØªØ¹Ø¯Ø§Ø¯ Ø¯Ø³ØªÙ‡â€ŒÙ‡Ø§ÛŒ Ú©Ù…
                    col_info['suggested_type'] = 'category'
                else:
                    col_info['suggested_type'] = 'numeric'

            elif any(keyword in col_lower for keyword in ['byte', 'packet', 'size', 'count', 'length']):
                col_info['suggested_type'] = 'numeric'

            elif any(keyword in col_lower for keyword in ['ratio', 'rate', 'percent', 'probability']):
                col_info['suggested_type'] = 'numeric_float'

            else:
                # ØªØ´Ø®ÛŒØµ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¯Ø§Ø¯Ù‡
                if pd.api.types.is_numeric_dtype(df[col]):
                    col_info['suggested_type'] = 'numeric'
                elif df[col].nunique() / len(df) < 0.1:  # Ú©Ù…ØªØ± Ø§Ø² 10% Ù…Ù‚Ø§Ø¯ÛŒØ± ÛŒÚ©ØªØ§
                    col_info['suggested_type'] = 'category'
                else:
                    col_info['suggested_type'] = 'string'

            column_analysis[col] = col_info

        return column_analysis


# ØªØ§Ø¨Ø¹ main Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø²Ú¯Ø§Ø±ÛŒ Ø¨Ø§ Ú©Ø¯ Ù…ÙˆØ¬ÙˆØ¯
def main(data_path=None, output_dir=None, allow_negative_cols=None, outlier_method='mark'):
    """ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ø¬Ø±Ø§ÛŒ Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ - Ø³Ø§Ø²Ú¯Ø§Ø± Ø¨Ø§ main.py Ø§ØµÙ„ÛŒ"""
    print("=" * 60)
    print("ÙØ§Ø² 1: Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ùˆ Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§")
    print("=" * 60)

    # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø³ÛŒØ± Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ø§Ú¯Ø± Ø¯Ø§Ø¯Ù‡â€ŒØ§ÛŒ Ø§Ø±Ø§Ø¦Ù‡ Ù†Ø´Ø¯Ù‡
    if data_path is None:
        data_path = "../data/network_logs.csv"

    # Ø§Ú¯Ø± Ù…Ø³ÛŒØ± Ù†Ø³Ø¨ÛŒ Ø§Ø³ØªØŒ Ø¢Ù† Ø±Ø§ Ø¨Ù‡ Ù…Ø³ÛŒØ± Ú©Ø§Ù…Ù„ ØªØ¨Ø¯ÛŒÙ„ Ú©Ù†
    if not os.path.isabs(data_path):
        # ÙØ±Ø¶ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… ÙØ§ÛŒÙ„ Ø¯Ø± Ù¾ÙˆØ´Ù‡ data Ù‚Ø±Ø§Ø± Ø¯Ø§Ø±Ø¯
        base_dir = Path(__file__).parent.parent
        data_path = base_dir / "data" / data_path

    # Ø§Ú¯Ø± output_dir Ù…Ø´Ø®Øµ Ù†Ø´Ø¯Ù‡ØŒ Ø§Ø² Ù¾ÙˆØ´Ù‡ data Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†
    if output_dir is None:
        output_dir = Path(__file__).parent.parent / "data/cleanedData"

    print(f"ğŸ“ ÙØ§ÛŒÙ„ ÙˆØ±ÙˆØ¯ÛŒ: {data_path}")
    print(f"ğŸ“‚ Ù¾ÙˆØ´Ù‡ Ø®Ø±ÙˆØ¬ÛŒ: {output_dir}")
    print(f"ğŸ¯ Ø±ÙˆØ´ Ù…Ø¯ÛŒØ±ÛŒØª outliers: {outlier_method}")
    if allow_negative_cols:
        print(f"âœ… Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù…Ø¬Ø§Ø² Ø¨Ø±Ø§ÛŒ Ù…Ù‚Ø§Ø¯ÛŒØ± Ù…Ù†ÙÛŒ: {allow_negative_cols}")

    try:
        # Ø§ÛŒØ¬Ø§Ø¯ Ù†Ù…ÙˆÙ†Ù‡ DataCleaner Ùˆ Ø§Ø¬Ø±Ø§ÛŒ Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ
        cleaner = DataCleaner(str(data_path), str(output_dir))
        result = cleaner.clean(
            outlier_method=outlier_method,
            allow_negative_cols=allow_negative_cols
        )

        # Ú¯Ø²Ø§Ø±Ø´ Ù†ØªÛŒØ¬Ù‡
        if result['success']:
            print("\n" + "=" * 50)
            print("âœ… ÙØ§Ø² 1 Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ù‡ Ù¾Ø§ÛŒØ§Ù† Ø±Ø³ÛŒØ¯!")
            print(f"ğŸ“Š Ø¯Ø§Ø¯Ù‡ Ù¾Ø§Ú©â€ŒØ´Ø¯Ù‡: {result['data_path']}")
            print(f"ğŸ“ˆ Ú¯Ø²Ø§Ø±Ø´: {result['report_path']}")
            print("=" * 50)
        else:
            print(f"\nâŒ ÙØ§Ø² 1 Ø¨Ø§ Ø®Ø·Ø§ Ù…ÙˆØ§Ø¬Ù‡ Ø´Ø¯: {result.get('error', 'Ø®Ø·Ø§ÛŒ Ù†Ø§Ø´Ù†Ø§Ø®ØªÙ‡')}")

        return result

    except Exception as e:
        error_result = {
            'success': False,
            'error': f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø¬Ø±Ø§ÛŒ ÙØ§Ø² 1: {str(e)}"
        }
        print(f"âŒ {error_result['error']}")
        return error_result


if __name__ == "__main__":
    # Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø§Ø² Ø¢Ø±Ú¯ÙˆÙ…Ø§Ù†â€ŒÙ‡Ø§ÛŒ Ø®Ø· ÙØ±Ù…Ø§Ù† (Ø¨Ø±Ø§ÛŒ Ø§Ø¬Ø±Ø§ÛŒ Ù…Ø³ØªÙ‚Ù„)
    import argparse

    parser = argparse.ArgumentParser(description='Ù¾Ø§Ú©â€ŒÚ©Ù†Ù†Ø¯Ù‡ Ø¹Ù…ÙˆÙ…ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ - ÙØ§Ø² 1')
    parser.add_argument('--input', type=str, help='Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„ ÙˆØ±ÙˆØ¯ÛŒ', default='network_logs.csv')
    parser.add_argument('--output', type=str, help='Ù¾ÙˆØ´Ù‡ Ø®Ø±ÙˆØ¬ÛŒ', default=None)
    parser.add_argument('--outlier-method', choices=['remove', 'mark', 'clip', 'ignore'],
                        default='remove', help='Ø±ÙˆØ´ Ù…Ø¯ÛŒØ±ÛŒØª outliers (Ù¾ÛŒØ´â€ŒÙØ±Ø¶: remove)')
    # parser.add_argument('--allow-negative', nargs='*', help='Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù…Ø¬Ø§Ø² Ø¨Ø±Ø§ÛŒ Ù…Ù‚Ø§Ø¯ÛŒØ± Ù…Ù†ÙÛŒ', default=[])

    args = parser.parse_args()

    # ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ ØªØ§Ø¨Ø¹ main Ø¨Ø§ Ø¢Ø±Ú¯ÙˆÙ…Ø§Ù†â€ŒÙ‡Ø§ÛŒ Ø®Ø· ÙØ±Ù…Ø§Ù†
    main(
        data_path=args.input,
        output_dir=args.output,
        outlier_method=args.outlier_method,
        # allow_negative_cols=args.allow_negative
    )