import pandas as pd
import numpy as np
import json
from pathlib import Path
import sys
import os

# Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…Ø³ÛŒØ± Ù…Ø§Ú˜ÙˆÙ„â€ŒÙ‡Ø§
sys.path.append(os.path.join(os.path.dirname(__file__)))

from feature_engineers.smart_feature_engineer import SmartFeatureEngineer


class FeatureEngineeringPipeline:
    """Ù¾Ø§ÛŒÙ¾Ù„Ø§ÛŒÙ† Ø§ØµÙ„ÛŒ Ù…Ù‡Ù†Ø¯Ø³ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒ - Ø¹Ù…ÙˆÙ…ÛŒ Ùˆ Ù‚Ø§Ø¨Ù„ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…Ø¬Ø¯Ø¯"""

    def __init__(self, metadata_path, scaling_strategy='standard'):
        self.metadata_path = metadata_path
        self.scaling_strategy = scaling_strategy
        self.engineer = None
        self.is_fitted = False

    def fit_transform(self, df, target_column='Action'):
        """Ø¢Ù…ÙˆØ²Ø´ Ùˆ ØªØ¨Ø¯ÛŒÙ„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§"""
        print("ğŸš€ Ø´Ø±ÙˆØ¹ ÙØ±Ø¢ÛŒÙ†Ø¯ Ù…Ù‡Ù†Ø¯Ø³ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§")
        print("=" * 60)
        print(f"ğŸ” Ø¨Ø±Ø±Ø³ÛŒ Ø³ØªÙˆÙ† Ù‡Ø¯Ù '{target_column}':")
        print(f"   Ù†ÙˆØ¹ Ø¯Ø§Ø¯Ù‡: {df[target_column].dtype}")
        print(f"   Ù…Ù‚Ø§Ø¯ÛŒØ± Ù…Ù†Ø­ØµØ± Ø¨Ù‡ ÙØ±Ø¯: {df[target_column].unique()}")

        # Ø§ÛŒØ¬Ø§Ø¯ Ù…Ù‡Ù†Ø¯Ø³ ÙˆÛŒÚ˜Ú¯ÛŒ
        self.engineer = SmartFeatureEngineer(
            metadata_path=self.metadata_path,
            target_column=target_column,
            scaling_strategy=self.scaling_strategy
        )

        # Ø§Ø¬Ø±Ø§ÛŒ Ù…Ù‡Ù†Ø¯Ø³ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒ
        df_engineered = self.engineer.fit_transform(df)
        self.is_fitted = True

        print(f"ğŸ” Ø³ØªÙˆÙ† Ù‡Ø¯Ù Ù¾Ø³ Ø§Ø² Ù¾Ø±Ø¯Ø§Ø²Ø´:")
        print(f"   Ù†ÙˆØ¹ Ø¯Ø§Ø¯Ù‡: {df_engineered[target_column].dtype}")
        print(f"   Ù…Ù‚Ø§Ø¯ÛŒØ± Ù…Ù†Ø­ØµØ± Ø¨Ù‡ ÙØ±Ø¯: {df_engineered[target_column].unique()}")

        # Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù†ØªØ§ÛŒØ¬
        self._save_results(df_engineered)

        return df_engineered

    def transform(self, df):
        """ØªØ¨Ø¯ÛŒÙ„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯"""
        if not self.is_fitted or self.engineer is None:
            raise Exception("Ø§Ø¨ØªØ¯Ø§ Ø¨Ø§ÛŒØ¯ Ù…ØªØ¯ fit_transform ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ Ø´ÙˆØ¯")

        return self.engineer.transform(df)

    def _convert_to_serializable(self, obj):
        """ØªØ¨Ø¯ÛŒÙ„ Ø§Ù†ÙˆØ§Ø¹ ØºÛŒØ±Ù‚Ø§Ø¨Ù„ Ø³Ø±ÛŒØ§Ù„Ø§ÛŒØ² Ø¨Ù‡ Ø§Ù†ÙˆØ§Ø¹ Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯ Ù¾Ø§ÛŒØªÙˆÙ†"""
        if isinstance(obj, (np.integer, np.int32, np.int64)):
            return int(obj)
        elif isinstance(obj, (np.floating, np.float32, np.float64)):
            return float(obj)
        elif isinstance(obj, (np.bool_)):
            return bool(obj)
        elif isinstance(obj, (np.ndarray)):
            return obj.tolist()
        elif isinstance(obj, pd.Timestamp):
            return obj.isoformat()
        elif isinstance(obj, dict):
            return {key: self._convert_to_serializable(value) for key, value in obj.items()}
        elif isinstance(obj, list):
            return [self._convert_to_serializable(item) for item in obj]
        elif isinstance(obj, tuple):
            return tuple(self._convert_to_serializable(item) for item in obj)
        elif pd.isna(obj):
            return None
        else:
            return obj

    def _save_results(self, df_final):
        """Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù†ØªØ§ÛŒØ¬ Ùˆ Ú¯Ø²Ø§Ø±Ø´"""
        print("\nğŸ’¾ Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù†ØªØ§ÛŒØ¬...")

        # Ø§ÛŒØ¬Ø§Ø¯ Ù¾ÙˆØ´Ù‡ Ø®Ø±ÙˆØ¬ÛŒ
        output_dir = Path(__file__).parent.parent / "data/engineeredData"
        output_dir.mkdir(exist_ok=True)

        # Ø°Ø®ÛŒØ±Ù‡ dataset Ù†Ù‡Ø§ÛŒÛŒ
        output_path = output_dir / "engineered_dataset.csv"
        df_final.to_csv(output_path, index=False)
        print(f"   âœ… dataset Ù†Ù‡Ø§ÛŒÛŒ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {output_path}")
        print(f"Ù…Ù‚Ø§Ø¯ÛŒØ± null: {df_final.isnull().sum().sum()}")

        # Ø§ÛŒØ¬Ø§Ø¯ Ú¯Ø²Ø§Ø±Ø´
        report = self._generate_engineering_report(df_final)
        report_path = output_dir / "feature_engineering_report.json"

        # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Ø§Ù†ÙˆØ§Ø¹ Ù‚Ø§Ø¨Ù„ Ø³Ø±ÛŒØ§Ù„Ø§ÛŒØ²
        serializable_report = self._convert_to_serializable(report)

        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(serializable_report, f, indent=2, ensure_ascii=False)

        print(f"   âœ… Ú¯Ø²Ø§Ø±Ø´ Ù…Ù‡Ù†Ø¯Ø³ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {report_path}")

        # Ø°Ø®ÛŒØ±Ù‡ pipeline (Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¢ÛŒÙ†Ø¯Ù‡)
        pipeline_info = {
            'metadata_path': str(self.metadata_path),
            'scaling_strategy': self.scaling_strategy,
            'fitted': self.is_fitted,
            'feature_summary': self.engineer.get_feature_summary() if self.engineer else None
        }

        # ØªØ¨Ø¯ÛŒÙ„ pipeline info Ø¨Ù‡ Ø§Ù†ÙˆØ§Ø¹ Ù‚Ø§Ø¨Ù„ Ø³Ø±ÛŒØ§Ù„Ø§ÛŒØ²
        serializable_pipeline_info = self._convert_to_serializable(pipeline_info)

        pipeline_path = output_dir / "feature_pipeline_info.json"
        with open(pipeline_path, 'w', encoding='utf-8') as f:
            json.dump(serializable_pipeline_info, f, indent=2, ensure_ascii=False)

        print(f"   âœ… Ø§Ø·Ù„Ø§Ø¹Ø§Øª pipeline Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {pipeline_path}")

        return output_path, report_path

    def _generate_engineering_report(self, df_final):
        """ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´ Ù…Ù‡Ù†Ø¯Ø³ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒ"""
        if self.engineer is None:
            return {}

        feature_summary = self.engineer.get_feature_summary()

        # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ù‚Ø§Ø¨Ù„ Ø³Ø±ÛŒØ§Ù„Ø§ÛŒØ² Ø¨ÙˆØ¯Ù† ØªÙˆØ²ÛŒØ¹ Ù‡Ø¯Ù
        target_dist = {}
        if self.engineer.target_column in df_final.columns:
            target_counts = df_final[self.engineer.target_column].value_counts()
            target_dist = {str(k): int(v) for k, v in target_counts.items()}

        report = {
            "metadata": {
                "phase": "Feature Engineering - Phase 2",
                "timestamp": pd.Timestamp.now().isoformat(),
                "final_shape": {
                    "rows": int(df_final.shape[0]),
                    "columns": int(df_final.shape[1])
                },
                "scaling_strategy": self.scaling_strategy,
                "target_column": self.engineer.target_column,
                "is_fitted": self.is_fitted
            },
            "feature_analysis": {
                "total_features": int(len(df_final.columns)),
                "numeric_features": int(len(df_final.select_dtypes(include=[np.number]).columns)),
                "categorical_features": int(len(df_final.select_dtypes(include=['object', 'category']).columns)),
                "target_distribution": target_dist
            },
            "engineer_summary": feature_summary,
            "data_quality": {
                "missing_values": int(df_final.isnull().sum().sum()),
                "memory_usage_mb": float(round(df_final.memory_usage(deep=True).sum() / 1024 / 1024, 2))
            },
            "process_summary": {
                "nat_features_created": len([col for col in df_final.columns if 'nat' in col.lower()]),
                "port_features_created": len([col for col in df_final.columns if 'port' in col.lower()]),
                "traffic_features_created": len(
                    [col for col in df_final.columns if any(x in col.lower() for x in ['byte', 'packet', 'ratio'])]),
                "time_features_created": len(
                    [col for col in df_final.columns if any(x in col.lower() for x in ['time', 'duration', 'second'])]),
                "log_features_created": len([col for col in df_final.columns if 'log1p' in col.lower()])
            }
        }

        return report


# ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ø¬Ø±Ø§ÛŒ Ù…Ø³ØªÙ‚Ù„
def main(input_file=None, metadata_file=None, scaling_strategy='standard'):
    """ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ Ø§Ø¬Ø±Ø§ÛŒ ÙØ§Ø² Û²"""
    if input_file is None:
        input_file = "../data/cleaned_network_data.csv"
    if metadata_file is None:
        metadata_file = "../data/columns_metadata.json"

    print("=" * 60)
    print("ÙØ§Ø² Û²: Ù…Ù‡Ù†Ø¯Ø³ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ùˆ Ø¹Ù…ÙˆÙ…ÛŒ")
    print("=" * 60)

    try:
        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø§Ú©â€ŒØ´Ø¯Ù‡
        df = pd.read_csv(input_file)
        print(f"ğŸ“ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø§Ú©â€ŒØ´Ø¯Ù‡ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯: {df.shape}")
        print(f"Ù…Ù‚Ø§Ø¯ÛŒØ± null: {df.isnull().sum().sum()}")

        # Ø§ÛŒØ¬Ø§Ø¯ Ùˆ Ø§Ø¬Ø±Ø§ÛŒ Ù¾Ø§ÛŒÙ¾Ù„Ø§ÛŒÙ†
        pipeline = FeatureEngineeringPipeline(
            metadata_path=metadata_file,
            scaling_strategy=scaling_strategy
        )

        df_engineered = pipeline.fit_transform(df)

        print(f"\nğŸ‰ ÙØ§Ø² Û² completed Ø´Ø¯!")
        print(f"ğŸ“Š Ø§Ø¨Ø¹Ø§Ø¯ Ù†Ù‡Ø§ÛŒÛŒ: {df_engineered.shape}")

        # Ù†Ù…Ø§ÛŒØ´ Ø®Ù„Ø§ØµÙ‡â€ŒØ§ÛŒ Ø§Ø² ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯Ù‡
        print(f"\nğŸ“ˆ Ø®Ù„Ø§ØµÙ‡ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯Ù‡:")
        print(f"   â€¢ Ú©Ù„ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§: {len(df_engineered.columns)}")
        print(f"   â€¢ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ: {len(df_engineered.select_dtypes(include=[np.number]).columns)}")
        print(f"   â€¢ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ: {len(df_engineered.select_dtypes(include=['object', 'category']).columns)}")

        # Ù†Ù…Ø§ÛŒØ´ ØªÙˆØ²ÛŒØ¹ Ù‡Ø¯Ù
        if 'Action' in df_engineered.columns:
            target_dist = df_engineered['Action'].value_counts()
            print(f"   â€¢ ØªÙˆØ²ÛŒØ¹ Ú©Ù„Ø§Ø³ Ù‡Ø¯Ù:")
            for cls, count in target_dist.items():
                print(f"     {cls}: {count} ({count / len(df_engineered) * 100:.1f}%)")

        return df_engineered, pipeline

    except Exception as e:
        print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø¬Ø±Ø§ÛŒ ÙØ§Ø² Û²: {e}")
        import traceback
        traceback.print_exc()
        return None, None


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description='ÙØ§Ø² Û²: Ù…Ù‡Ù†Ø¯Ø³ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯')
    parser.add_argument('--input', type=str, help='ÙØ§ÛŒÙ„ ÙˆØ±ÙˆØ¯ÛŒ', default='../data/cleanedData/network_logs_cleaned.csv')
    parser.add_argument('--metadata', type=str, help='ÙØ§ÛŒÙ„ Ù…ØªØ§Ø¯ÛŒØªØ§', default='../data/columns_metadata.json')
    parser.add_argument('--scaling', choices=['standard', 'robust'],
                        default='standard', help='Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ Ù…Ù‚ÛŒØ§Ø³â€ŒØ¨Ù†Ø¯ÛŒ')

    args = parser.parse_args()

    main(input_file=args.input, metadata_file=args.metadata, scaling_strategy=args.scaling)