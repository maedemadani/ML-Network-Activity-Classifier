from typing import Any, Dict, Tuple

import pandas as pd
import numpy as np
import json
from pathlib import Path
import sys
import os
from datetime import datetime

# Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…Ø³ÛŒØ± Ù…Ø§Ú˜ÙˆÙ„â€ŒÙ‡Ø§
sys.path.append(os.path.join(os.path.dirname(__file__), 'model_manager'))
sys.path.append(os.path.join(os.path.dirname(__file__), 'config'))

from model_manager.model_trainer import ModelTrainer
from model_manager.model_evaluator import ModelEvaluator
from config.model_config import Phase4Config


class Phase4ModelTraining:
    """Ú©Ù„Ø§Ø³ Ø§ØµÙ„ÛŒ ÙØ§Ø² Û´ - Ù…Ø¯ÛŒØ±ÛŒØª Ú©Ø§Ù…Ù„ Ù…Ø¯Ù„â€ŒØ³Ø§Ø²ÛŒ Ùˆ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ"""

    def __init__(self, config=None):
        self.config = config or Phase4Config()
        self.trainer = ModelTrainer(self.config)
        self.evaluator = ModelEvaluator(self.config)
        self.results = {}

    def run_training_pipeline(self, data_dir: str = "data/balancedData") -> Dict[str, Any]:
        """Ø§Ø¬Ø±Ø§ÛŒ Ú©Ø§Ù…Ù„ Ù¾Ø§ÛŒÙ¾Ù„Ø§ÛŒÙ† ÙØ§Ø² Û´"""
        print("=" * 60)
        print("ÙØ§Ø² Û´: Ù…Ø¯Ù„â€ŒØ³Ø§Ø²ÛŒ Ùˆ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ")
        print("=" * 60)

        data_path = Path(data_dir)

        try:
            # Û±. Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Train
            print("ğŸ“¥ Ù…Ø±Ø­Ù„Ù‡ Û±: Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´...")
            train_strategies = self._load_train_data(data_path)

            # Û². Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Test
            print("\nğŸ“Š Ù…Ø±Ø­Ù„Ù‡ Û²: Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ³Øª...")
            X_test, y_test = self._load_test_data(data_path)

            # Û³. Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù¾Ø§ÛŒÙ‡
            print("\nğŸ¯ Ù…Ø±Ø­Ù„Ù‡ Û³: Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù¾Ø§ÛŒÙ‡...")
            base_models = {}
            for strategy_name, (X_train, y_train) in train_strategies.items():
                base_models[strategy_name] = self.trainer.train_base_models(X_train, y_train, strategy_name)

            print("\nğŸ’¾ Ù…Ø±Ø­Ù„Ù‡ Ûµ: Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù¾Ø§ÛŒÙ‡ Ø¨Ø±Ø§ÛŒ Ù‡Ù…Ù‡ Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒâ€ŒÙ‡Ø§...")
            for strategy_name in train_strategies.keys():
                self.trainer.save_models(base_models, {}, strategy_name)

            # âš™ï¸ Ù…Ø±Ø­Ù„Ù‡ Û´: ØªÙ†Ø¸ÛŒÙ… Ù‡Ø§ÛŒÙ¾Ø±Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ ÙÙ‚Ø· Ø¨Ø±Ø§ÛŒ RF Ùˆ SVM Ø±ÙˆÛŒ oversampled Ø¯Ø§Ø¯Ù‡
            print("\nâš™ï¸  Ù…Ø±Ø­Ù„Ù‡ Û´: ØªÙ†Ø¸ÛŒÙ… Ù‡Ø§ÛŒÙ¾Ø±Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ (ÙÙ‚Ø· RF Ùˆ SVM Ø±ÙˆÛŒ Oversampling)...")
            if 'oversampled' in train_strategies:
                X_train_over, y_train_over = train_strategies['oversampled']
                tuned_models, tuning_results = self.trainer.hyperparameter_tuning(
                    X_train_over, y_train_over, 'oversampled',
                    model_names=['svm', 'random_forest']  # âœ… Ù…Ø­Ø¯ÙˆØ¯ Ø¨Ù‡ Ø¯Ùˆ Ù…Ø¯Ù„
                )
                self.results['tuning_results'] = tuning_results

                # Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ ØªÙ†Ø¸ÛŒÙ…â€ŒØ´Ø¯Ù‡
                print("\nğŸ’¾ Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ ØªÙ†Ø¸ÛŒÙ…â€ŒØ´Ø¯Ù‡...")
                self.trainer.save_models({}, tuned_models, 'oversampled')
            else:
                tuned_models = {}
                print("   âš ï¸  Ø¯Ø§Ø¯Ù‡ Oversampling Ø¨Ø±Ø§ÛŒ ØªÙ†Ø¸ÛŒÙ… ÛŒØ§ÙØª Ù†Ø´Ø¯")

            # ğŸ“ˆ Ù…Ø±Ø­Ù„Ù‡ Û¶: Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø±ÙˆÛŒ ØªØ³Øª set + Ø±Ø³Ù… Ù…Ø§ØªØ±ÛŒØ³ Ø³Ø±Ø¯Ø±Ú¯Ù…ÛŒ
            print("\nğŸ“ˆ Ù…Ø±Ø­Ù„Ù‡ Û¶: Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø±ÙˆÛŒ ØªØ³Øª set...")
            evaluation_results = {}
            for strategy_name in train_strategies.keys():
                strategy_eval = self.evaluator.evaluate_models(X_test, y_test, strategy_name)
                # âœ… Ø±Ø³Ù… Ù…Ø§ØªØ±ÛŒØ³ Ø³Ø±Ø¯Ø±Ú¯Ù…ÛŒ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ù…Ø¯Ù„
                self.evaluator.plot_confusion_matrices(strategy_eval, y_test, strategy_name)
                evaluation_results[strategy_name] = strategy_eval

            self.results['evaluation_results'] = evaluation_results

            # Û·. ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´â€ŒÙ‡Ø§
            print("\nğŸ“‹ Ù…Ø±Ø­Ù„Ù‡ Û·: ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´â€ŒÙ‡Ø§ÛŒ Ù†Ù‡Ø§ÛŒÛŒ...")
            output_dir = Path(self.config.MODEL_SAVE_PATHS['evaluation'])
            self.evaluator.generate_reports(output_dir)

            # Û¸. Ø§Ù†ØªØ®Ø§Ø¨ Ù…Ø¯Ù„ Ø¨Ø±ØªØ±
            print("\nğŸ† Ù…Ø±Ø­Ù„Ù‡ Û¸: Ø§Ù†ØªØ®Ø§Ø¨ Ù…Ø¯Ù„ Ø¨Ø±ØªØ±...")
            best_model_info = self._select_best_model(evaluation_results)
            self.results['best_model'] = best_model_info

            print(f"\nğŸ‰ ÙØ§Ø² Û´ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª completed Ø´Ø¯!")
            print(f"ğŸ“ Ù†ØªØ§ÛŒØ¬ Ø¯Ø±: {output_dir}")

            return self.results

        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø¬Ø±Ø§ÛŒ ÙØ§Ø² Û´: {e}")
            import traceback
            traceback.print_exc()
            return {}

    def _load_train_data(self, data_path: Path) -> Dict[str, Tuple[pd.DataFrame, pd.Series]]:
        """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ø§Ø² Ù¾ÙˆØ´Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯"""
        train_data = {}
        # detect all subfolders except 'test'
        for strategy_path in data_path.iterdir():
            if not strategy_path.is_dir() or strategy_path.name.lower() == "test":
                continue
            X_path = strategy_path / "X_train.csv"
            y_path = strategy_path / "y_train.csv"
            if X_path.exists() and y_path.exists():
                X_train = pd.read_csv(X_path)
                y_train = pd.read_csv(y_path).iloc[:, 0]
                train_data[strategy_path.name] = (X_train, y_train)
                print(f"   âœ… {strategy_path.name}: {X_train.shape}")
        return train_data

    def _load_test_data(self, data_path: Path) -> Tuple[pd.DataFrame, pd.Series]:
        """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ³Øª (Ù¾ÙˆØ´Ù‡â€ŒÛŒ test ÛŒØ§ Ø²ÛŒØ±Ù¾ÙˆØ´Ù‡ Ù…Ø´Ø§Ø¨Ù‡)"""
        # look for any folder containing X_test.csv
        for folder in data_path.rglob("X_test.csv"):
            test_path = folder.parent
            break
        else:
            raise FileNotFoundError("âŒ Ù‡ÛŒÚ† ÙØ§ÛŒÙ„ X_test.csv ÛŒØ§ÙØª Ù†Ø´Ø¯ Ø¯Ø± Ù…Ø³ÛŒØ± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§")

        X_test = pd.read_csv(test_path / "X_test.csv")
        y_test = pd.read_csv(test_path / "y_test.csv").iloc[:, 0]
        print(f"   âœ… Test set found in: {test_path}")
        print(f"   âœ… Test set shape: {X_test.shape}")
        return X_test, y_test

    def _select_best_model(self, evaluation_results: Dict[str, Any]) -> Dict[str, Any]:
        """Ø§Ù†ØªØ®Ø§Ø¨ Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ø§Ù…Ù†ÛŒØªÛŒ"""
        best_model = None
        best_score = -1
        best_details = {}

        for strategy_name, models_eval in evaluation_results.items():
            for model_name, eval_result in models_eval.items():
                if 'security_metrics' in eval_result:
                    security_score = (
                            eval_result['security_metrics']['mean_security_recall'] * 0.4 +
                            eval_result['security_metrics']['security_f1'] * 0.3 +
                            eval_result['security_metrics']['threat_detection_rate'] * 0.3
                    )

                    if security_score > best_score:
                        best_score = security_score
                        best_model = f"{strategy_name}_{model_name}"
                        best_details = {
                            'strategy': strategy_name,
                            'model': model_name,
                            'security_score': security_score,
                            'security_recall': eval_result['security_metrics']['mean_security_recall'],
                            'security_f1': eval_result['security_metrics']['security_f1'],
                            'threat_detection_rate': eval_result['security_metrics']['threat_detection_rate'],
                            'accuracy': eval_result['general_metrics']['accuracy'],
                            'f1_macro': eval_result['general_metrics']['f1_macro']
                        }

        print(f"   ğŸ† Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„: {best_model}")
        print(f"   ğŸ“Š Ø§Ù…ØªÛŒØ§Ø² Ø§Ù…Ù†ÛŒØªÛŒ: {best_score:.3f}")
        print(f"   ğŸ›¡ï¸  Recall Ø§Ù…Ù†ÛŒØªÛŒ: {best_details['security_recall']:.3f}")

        return best_details


def main():
    """ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ Ø§Ø¬Ø±Ø§ÛŒ ÙØ§Ø² Û´"""
    print("ğŸš€ Ø§Ø¬Ø±Ø§ÛŒ ÙØ§Ø² Û´: Ù…Ø¯Ù„â€ŒØ³Ø§Ø²ÛŒ Ùˆ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ")

    try:
        # Ø§ÛŒØ¬Ø§Ø¯ Ù…Ø¯Ù„â€ŒØ³Ø§Ø²
        model_training = Phase4ModelTraining()

        # Ø§Ø¬Ø±Ø§ÛŒ Ù¾Ø§ÛŒÙ¾Ù„Ø§ÛŒÙ†
        results = model_training.run_training_pipeline()

        if results:
            print(f"\nğŸ“Š Ø®Ù„Ø§ØµÙ‡ Ù†ØªØ§ÛŒØ¬ ÙØ§Ø² Û´:")
            print(f"   â€¢ Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ø¯ÛŒØ¯Ù‡: {list(results.get('evaluation_results', {}).keys())}")

            best_model = results.get('best_model', {})
            if best_model:
                print(f"   â€¢ Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„: {best_model['strategy']}_{best_model['model']}")
                print(f"   â€¢ Ø§Ù…ØªÛŒØ§Ø² Ø§Ù…Ù†ÛŒØªÛŒ: {best_model['security_score']:.3f}")
                print(f"   â€¢ Recall Ø§Ù…Ù†ÛŒØªÛŒ: {best_model['security_recall']:.3f}")

            return results
        else:
            print("âŒ ÙØ§Ø² Û´ Ø¨Ø§ Ø®Ø·Ø§ Ù…ÙˆØ§Ø¬Ù‡ Ø´Ø¯")
            return None

    except Exception as e:
        print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø¬Ø±Ø§ÛŒ ÙØ§Ø² Û´: {e}")
        import traceback
        traceback.print_exc()
        return None


if __name__ == "__main__":
    main()