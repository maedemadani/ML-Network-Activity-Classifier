import pandas as pd
import numpy as np
from typing import Dict, List, Any
from scipy import stats


class DataValidator:
    """Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ØªØ¹Ø§Ø¯Ù„â€ŒØ´Ø¯Ù‡"""

    def __init__(self, config):
        self.config = config
        self.validation_reports = {}

    def validate_balanced_data(self, original_data: Dict[str, Any],
                               balanced_data: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:
        """Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ø¬Ø§Ù…Ø¹ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ØªØ¹Ø§Ø¯Ù„â€ŒØ´Ø¯Ù‡"""
        validation_results = {}

        for strategy_name, data_dict in balanced_data.items():
            print(f"ðŸ” Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ: {strategy_name}")

            X = data_dict['X']
            y = data_dict['y']

            actual_has_nulls = X.isnull().sum().sum() > 0 or y.isnull().sum().sum() > 0

            validation = {
                'basic_checks': self._basic_validation(X, y),
                'distribution_checks': self._distribution_validation(original_data['y'], y),
                'quality_checks': self._quality_validation(X, y, strategy_name),
                'leakage_checks': self._leakage_validation(X, original_data['X']),
                'actual_null_status': {
                    'x_nulls': int(X.isnull().sum().sum()),
                    'y_nulls': int(y.isnull().sum().sum()),
                    'has_nulls_actual': actual_has_nulls
                }
            }

            validation_results[strategy_name] = validation

        return validation_results

    def _basic_validation(self, X: pd.DataFrame, y: pd.Series) -> Dict[str, Any]:
        """Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ù¾Ø§ÛŒÙ‡"""
        has_nulls = X.isnull().sum().sum() > 0 or y.isnull().sum() > 0

        checks = {
            'has_nulls': has_nulls,
            'shape_consistent': len(X) == len(y),
            'data_types_consistent': all(X.dtypes != 'object'),
            'class_diversity': y.nunique() > 1
        }

        checks['all_passed'] = all([
            not checks['has_nulls'],  # Ù†Ø¨Ø§ÛŒØ¯ null Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯
            checks['shape_consistent'],  # shapes Ø¨Ø§ÛŒØ¯ match Ø¨Ø§Ø´Ù†Ø¯
            checks['data_types_consistent'],  # Ø¨Ø§ÛŒØ¯ Ø¹Ø¯Ø¯ÛŒ Ø¨Ø§Ø´Ø¯
            checks['class_diversity']  # Ø¨Ø§ÛŒØ¯ Ø¨ÛŒØ´ Ø§Ø² ÛŒÚ© Ú©Ù„Ø§Ø³ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯
        ])

        print(f"   âœ… Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ: has_nulls={has_nulls}, all_passed={checks['all_passed']}")

        return checks

    def _distribution_validation(self, y_original: pd.Series, y_balanced: pd.Series) -> Dict[str, Any]:
        """Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ ØªÙˆØ²ÛŒØ¹ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§"""
        orig_counts = y_original.value_counts()
        balanced_counts = y_balanced.value_counts()

        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ø¢Ù…Ø§Ø±ÛŒ
        distribution_metrics = {
            'original_imbalance_ratio': orig_counts.max() / orig_counts.min(),
            'balanced_imbalance_ratio': balanced_counts.max() / balanced_counts.min(),
            'improvement_ratio': (orig_counts.max() / orig_counts.min()) / (
                        balanced_counts.max() / balanced_counts.min()),
            'class_preservation': set(orig_counts.index) == set(balanced_counts.index),
            'min_samples_per_class': balanced_counts.min() >= 10  # Ø­Ø¯Ø§Ù‚Ù„ 10 Ù†Ù…ÙˆÙ†Ù‡ per class
        }

        return distribution_metrics

    def _quality_validation(self, X: pd.DataFrame, y: pd.Series, strategy_name: str) -> Dict[str, Any]:
        """Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ú©ÛŒÙÛŒØª Ø¯Ø§Ø¯Ù‡"""
        if strategy_name == 'original':
            return {'synthetic_samples': 0, 'quality_score': 1.0}

        # Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ØªØ¹Ø§Ø¯Ù„â€ŒØ´Ø¯Ù‡ØŒ Ø¨Ø±Ø±Ø³ÛŒ Ú©ÛŒÙÛŒØª ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§
        quality_metrics = {
            'feature_variance': X.var().mean(),  # Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† ÙˆØ§Ø±ÛŒØ§Ù†Ø³ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§
            'correlation_structure': X.corr().abs().mean().mean(),  # Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ
            'outlier_ratio': self._calculate_outlier_ratio(X),
            'synthetic_quality': 'unknown'  # Ø¨Ø±Ø§ÛŒ SMOTE Ù†ÛŒØ§Ø² Ø¨Ù‡ ØªØ­Ù„ÛŒÙ„ Ù¾ÛŒØ´Ø±ÙØªÙ‡â€ŒØªØ±
        }

        return quality_metrics

    def _leakage_validation(self, X_balanced: pd.DataFrame, X_original: pd.DataFrame) -> Dict[str, Any]:
        """Ø¨Ø±Ø±Ø³ÛŒ Ù†Ø´Øª Ø¯Ø§Ø¯Ù‡"""
        # Ø¨Ø±Ø±Ø³ÛŒ overlap Ø¨ÛŒÙ† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ØªØ¹Ø§Ø¯Ù„ Ùˆ Ø§ØµÙ„ÛŒ
        if len(X_balanced) <= len(X_original):
            # Ø¨Ø±Ø§ÛŒ undersamplingØŒ Ø¨Ø§ÛŒØ¯ Ø²ÛŒØ±Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¨Ø§Ø´Ø¯
            is_subset = X_balanced.index.isin(X_original.index).all()
        else:
            # Ø¨Ø±Ø§ÛŒ oversamplingØŒ Ø¨Ø§ÛŒØ¯ Ø´Ø§Ù…Ù„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø§ØµÙ„ÛŒ Ø¨Ø§Ø´Ø¯
            is_subset = X_original.index.isin(X_balanced.index).all()

        return {
            'is_proper_subset': is_subset,
            'no_data_leakage': True  # Ø¨Ø± Ø§Ø³Ø§Ø³ Ø·Ø±Ø§Ø­ÛŒ Ù…Ø§
        }

    def _calculate_outlier_ratio(self, X: pd.DataFrame, threshold: float = 3.0) -> float:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù†Ø³Ø¨Øª outlierÙ‡Ø§"""
        z_scores = np.abs(stats.zscore(X.select_dtypes(include=[np.number])))
        outlier_mask = (z_scores > threshold).any(axis=1)
        return outlier_mask.mean()